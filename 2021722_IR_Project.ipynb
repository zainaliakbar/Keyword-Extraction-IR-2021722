{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uTesc7XpEMTi"
      },
      "id": "uTesc7XpEMTi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59cdadc0",
      "metadata": {
        "id": "59cdadc0"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9931c89a",
      "metadata": {
        "id": "9931c89a"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/papers.csv')\n",
        "df = df.iloc[:5000,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8828c01f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 866
        },
        "id": "8828c01f",
        "outputId": "02d87e8d-0dfe-4531-e52c-dae0b89a3086"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     id  year                                              title event_type  \\\n",
              "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
              "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
              "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
              "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
              "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
              "5  1002  1994  Using a neural net to instantiate a deformable...        NaN   \n",
              "6  1003  1994           Plasticity-Mediated Competitive Learning        NaN   \n",
              "7  1004  1994  ICEG Morphology Classification using an Analog...        NaN   \n",
              "8  1005  1994  Real-Time Control of a Tokamak Plasma Using Ne...        NaN   \n",
              "9  1006  1994  Pulsestream Synapses with Non-Volatile Analogu...        NaN   \n",
              "\n",
              "                                            pdf_name          abstract  \\\n",
              "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
              "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
              "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
              "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
              "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
              "5  1002-using-a-neural-net-to-instantiate-a-defor...  Abstract Missing   \n",
              "6  1003-plasticity-mediated-competitive-learning.pdf  Abstract Missing   \n",
              "7  1004-iceg-morphology-classification-using-an-a...  Abstract Missing   \n",
              "8  1005-real-time-control-of-a-tokamak-plasma-usi...  Abstract Missing   \n",
              "9  1006-pulsestream-synapses-with-non-volatile-an...  Abstract Missing   \n",
              "\n",
              "                                          paper_text  \n",
              "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
              "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
              "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
              "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
              "4  Neural Network Ensembles, Cross\\nValidation, a...  \n",
              "5  U sing a neural net to instantiate a\\ndeformab...  \n",
              "6  Plasticity-Mediated Competitive Learning\\n\\nTe...  \n",
              "7  ICEG Morphology Classification using an\\nAnalo...  \n",
              "8  Real-Time Control of a Tokamak Plasma\\nUsing N...  \n",
              "9  Real-Time Control of a Tokamak Plasma\\nUsing N...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ecdab3a9-8f4b-4f9e-adb2-8411e537409a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1002</td>\n",
              "      <td>1994</td>\n",
              "      <td>Using a neural net to instantiate a deformable...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1002-using-a-neural-net-to-instantiate-a-defor...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>U sing a neural net to instantiate a\\ndeformab...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1003</td>\n",
              "      <td>1994</td>\n",
              "      <td>Plasticity-Mediated Competitive Learning</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1003-plasticity-mediated-competitive-learning.pdf</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Plasticity-Mediated Competitive Learning\\n\\nTe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1004</td>\n",
              "      <td>1994</td>\n",
              "      <td>ICEG Morphology Classification using an Analog...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1004-iceg-morphology-classification-using-an-a...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>ICEG Morphology Classification using an\\nAnalo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1005</td>\n",
              "      <td>1994</td>\n",
              "      <td>Real-Time Control of a Tokamak Plasma Using Ne...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1005-real-time-control-of-a-tokamak-plasma-usi...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Real-Time Control of a Tokamak Plasma\\nUsing N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1006</td>\n",
              "      <td>1994</td>\n",
              "      <td>Pulsestream Synapses with Non-Volatile Analogu...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1006-pulsestream-synapses-with-non-volatile-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Real-Time Control of a Tokamak Plasma\\nUsing N...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ecdab3a9-8f4b-4f9e-adb2-8411e537409a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ecdab3a9-8f4b-4f9e-adb2-8411e537409a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ecdab3a9-8f4b-4f9e-adb2-8411e537409a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-241817e7-ed36-45dc-ab9a-d98f17d4d924\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-241817e7-ed36-45dc-ab9a-d98f17d4d924')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-241817e7-ed36-45dc-ab9a-d98f17d4d924 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5000,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1522,\n        \"min\": 1,\n        \"max\": 5526,\n        \"num_unique_values\": 5000,\n        \"samples\": [\n          2365,\n          3345,\n          3405\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7,\n        \"min\": 1987,\n        \"max\": 2014,\n        \"num_unique_values\": 26,\n        \"samples\": [\n          2000,\n          2006,\n          1987\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5000,\n        \"samples\": [\n          \"Large Scale Online Learning\",\n          \"The Value of Labeled and Unlabeled Examples when the Model is Imperfect\",\n          \"Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"event_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Oral\",\n          \"Spotlight\",\n          \"Poster\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5000,\n        \"samples\": [\n          \"2365-large-scale-online-learning.pdf\",\n          \"3345-the-value-of-labeled-and-unlabeled-examples-when-the-model-is-imperfect.pdf\",\n          \"3405-hierarchical-semi-markov-conditional-random-fields-for-recursive-sequential-data.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2167,\n        \"samples\": [\n          \"In this paper we seek to detect rectangular cuboids and localize their corners in uncalibrated single-view images depicting everyday scenes. In contrast to recent approaches that rely on detecting vanishing points of the scene and grouping line segments to form cuboids, we build a discriminative parts-based detector that models the appearance of the cuboid corners and internal edges while enforcing consistency to a 3D cuboid model. Our model is invariant to the different 3D viewpoints and aspect ratios and is able to detect cuboids across many different object categories. We introduce a database of images with cuboid annotations that spans a variety of indoor and outdoor scenes and show qualitative and quantitative results on our collected database. Our model out-performs baseline detectors that use 2D constraints alone on the task of localizing cuboid corners.\",\n          \"We introduce a new objective function for pool-based Bayesian active learning with probabilistic hypotheses. This objective function, called the policy Gibbs error, is the expected error rate of a random classifier drawn from the prior distribution on the examples adaptively selected by the active learning policy. Exact maximization of the policy Gibbs error is hard, so we propose a greedy strategy that maximizes the Gibbs error at each iteration, where the Gibbs error on an instance is the expected error of a random classifier selected from the posterior label distribution on that instance. We apply this maximum Gibbs error criterion to three active learning scenarios: non-adaptive, adaptive, and batch active learning. In each scenario, we prove that the criterion achieves near-maximal policy Gibbs error when constrained to a fixed budget. For practical implementations, we provide approximations to the maximum Gibbs error criterion for Bayesian conditional random fields and transductive Naive Bayes. Our experimental results on a named entity recognition task and a text classification task show that the maximum Gibbs error criterion is an effective active learning criterion for noisy models.\",\n          \"We present a dynamic nonlinear generative model for visual motion based on a latent representation of binary-gated Gaussian variables. Trained on sequences of images, the model learns to represent different movement directions in different variables. We use an online approximate-inference scheme that can be mapped to the dynamics of networks of neurons. Probed with drifting grating stimuli and moving bars of light, neurons in the model show patterns of responses analogous to those of direction-selective simple cells in primary visual cortex. Most model neurons also show speed tuning and respond equally well to a range of motion directions and speeds aligned to the constraint line of their respective preferred speed. We show how these computations are enabled by a specific pattern of recurrent connections learned by the model.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4998,\n        \"samples\": [\n          \"Visual gesture-based robot guidance\\nwith a modular neural system\\n\\nE. Littmann,\\n\\nA. Drees, and H. Ritter\\n\\nAbt. Neuroinformatik, Fak. f. Informatik\\nUniversitat Ulm, D-89069 Ulm, FRG\\nenno@neuro.informatik.uni-ulm.de\\n\\nAG Neuroinformatik, Techn. Fakultat\\nUniv. Bielefeld, D-33615 Bielefeld, FRG\\nandrea,helge@techfak.uni-bielefeld.de\\n\\nAbstract\\nWe report on the development of the modular neural system \\\"SEEEAGLE\\\" for the visual guidance of robot pick-and-place actions.\\nSeveral neural networks are integrated to a single system that visually recognizes human hand pointing gestures from stereo pairs\\nof color video images. The output of the hand recognition stage is\\nprocessed by a set of color-sensitive neural networks to determine\\nthe cartesian location of the target object that is referenced by the\\npointing gesture. Finally, this information is used to guide a robot\\nto grab the target object and put it at another location that can\\nbe specified by a second pointing gesture. The accuracy of the current system allows to identify the location of the referenced target\\nobject to an accuracy of 1 cm in a workspace area of 50x50 cm. In\\nour current environment, this is sufficient to pick and place arbitrarily positioned target objects within the workspace. The system\\nconsists of neural networks that perform the tasks of image segmentation, estimation of hand location, estimation of 3D-pointing\\ndirection, object recognition, and necessary coordinate transforms.\\nDrawing heavily on the use of learning algorithms, the functions of\\nall network modules were created from data examples only.\\n\\n1\\n\\nIntroduction\\n\\nThe rapidly developing technology in the fields of robotics and virtual reality requires the development of new and more powerful interfaces for configuration and\\ncontrol of such devices. These interfaces should be intuitive for the human advisor\\nand comfortable to use. Practical solutions so far require the human to wear a\\ndevice that can transfer the necessary information. One typical example is the data\\nglove [14, 12]. Clearly, in the long run solutions that are contactless will be much\\nmore desirable, and vision is one of the major modalities that appears especially\\nsuited for the realization of such solutions.\\nIn the present paper, we focus on a still restricted but very important task in robot\\ncontrol, the guidance of robot pick-and-place actions by unconstrained human pointing gestures in a realistic laboratory environment. The input of target locations by\\n\\n\\f904\\n\\nE. LITTMANN, A. DREES, H. RITTER\\n\\npointing gestures provides a powerful, very intuitive and comfortable functionality\\nfor a vision-based man-machine interface for guiding robots and extends previous\\nwork that focused on the detection of hand location or the discrimination of a small,\\ndiscrete number of hand gestures only [10, 1, 2, 8]. Besides two color cameras, no\\nspecial device is necessary to evaluate the gesture of the human operator.\\nA second goal of our approach is to investigate how to build a neural system for\\nsuch a complex task from several neural modules. The development of advanced\\nartificial neural systems challenges us with the task of finding architect.ures for the\\ncooperat.ion of multiple functional modules such that. part of the structure of the\\noverall system can be designed at a useful level of abstraction, but at the same t.ime\\nlearning can be used to create or fine-tune the functionality of parts of t.he system\\non the basis of suit.able training examples.\\nTo approach this goal requires to shift the focus from exploring t.he properties of\\nsingle networks to exploring the propert.ies of entire systems of neural networks.\\nThe work on \\\"mixtures of experts\\\" [3, 4] is one important contribution along these\\nlines. While this is a widely applicable and powerful approach, there clearly is\\na need to go beyond the exploration of strictly hierarchical systems and to gain\\nexperience with architectures t.hat admit more complex types of information flow\\nas required e.g. by the inclusion of feat.ures such as control of focal attention or\\nreent.rant processing branches. The need for such features arose very naturally in\\nthe context of the task described above, and in the following sect.ion we will report\\nour results wit.h a system architecture that is crucially based on the exploitation of\\nsuch elements.\\n\\n2\\n\\nSystem architecture\\n\\nOur system, described in fig. 1, is situated in a complex laboratory environment. A\\nrobot arm with manipulator is mounted at one side of a table with several objects\\nof different color placed on it. A human operator is positioned at the next side to\\nthe right of the robot. This scenery is watched by two cameras from the other two\\nsides from high above. The cameras yield a stereo color image of t.he scene (images\\n10). The operator points with one hand at one of the objects on the table. On the\\nbasis of the image information, the object is located and the robot grabs it. Then,\\nthe operator points at another location, where the robot releases the object. 1\\nThe syst.em consists of several hardware components: a PUMA 560 robot arm with\\nsix axes and a three-fingered manipulator 2; two single-chip PULNIX color cameras;\\ntwo ANDRox vision boards with software for data acquisition and processing; a\\nwork space consisting of a table with a black grid on a yellow surface. Robot and\\nperson refer to the same work space. Bot.h cameras must show both the human\\nhand and the table with the objects. Within this constraint, the position of the\\ncameras can be chosen freely as long as they yield significantly different views.\\nAn important prerequisite for the recognition of the pointing direction is the segmentation of the human hand from the background scenery. This task is solved by\\na LLM network (Sl) trained to yield a probability value for each image pixel to\\nbelong to the hand region. The training is based on t.he local color information.\\nThis procedure has been investigated in [7].\\nAn important feature of the chosen method is the great reliability and robustness\\nof both the classification performance and the localization accuracy of the searched\\nobject. Furthermore, the performance is quite constant over a wide range of image\\nresolutions. This allows a fast two-step procedure: First, the images are segmented\\nin low resolution (Sl: 11 -+ A1) and the hand position is extracted. Then, a small\\n1 In analogy to the sea eagle who watches its prey from high above, shoots down to grab\\nthe prey, and then flies to a safe place to feed, we nicknamed our system \\\"SEE-EAGLE\\\".\\n2Development by Prof. Pfeiffer, TV Munich\\n\\n\\fVisual Gesture-based Robot Guidance with a Modular Neural System\\n\\n905\\n\\nFig. 1: System architecture. From two color camera images 10 we extract the hand position\\n(11 I> Sl I> A1 (pixel coord.) I> P1 I> cartesian hand coord.). In a subframe centered on\\nthe hand location (12) we determine the pointing direction (12 I> S2 I> A2 (pixel coord.) I>\\nG I> D I> pointing angles). Pointing direction and hand location define a cartesian target\\nlocation that is mapped to image coord. that define the centers of object subframes (10 I>\\nP2 I> 13). There we determine the target object (13 I> S3 I> A3) and map the pixel coord.\\nof its centers to world coord. (A3 I> P3 I> world target loc.). These coordinates are used\\nto guide the robot R to the target object.\\n\\n\\f906\\n\\nE. LITTMANN. A. DREES. H. RlTIER\\n\\nsubframe (12) around the estimated hand position is processed in high resolution\\nby another dedicated LLM network (S2: 12 - t A2). For details of the segmentation\\nprocess, refer to [6].\\nThe extraction of hand information by LLMs on the basis of Gabor masks has\\nalready been studied for hand posture [9] and orientation [5]. The method is based\\non a segmented image containing the hand only (A2). This image is filtered by 36\\nGabor masks that are arranged on a 3x3 grid with 4 directions per grid position\\nand centered on the hand. The filter kernels have a radius of 10 pixels, the distance\\nbetween the grid points is 20 pixels. The 36 filter responses (G) form the input\\nvector for a LLM network (D). Further details of the processing are reported in [6].\\nThe network yields the pointing direction of the hand (D: 12 - t G - t pointing\\ndirection). Together with the hand position which is computed by a parametrized\\nself-organizing map (\\\"PSOM\\\", see below and [11, 13]) (P1: Al - t cartesian hand\\nposition), a (cartesian) target location in the workspace can be calculated. This\\nlocation can be retransformed by the PSOM into pixel coordinates (P2: cartesian\\ntarget location - t target pixel coordinates). These coordinates define the center of\\nan \\\"attention region\\\" (13) that is searched for a set of predefined target objects.\\nThis object recognition is performed by a set of LLM color segmentation networks\\n(S3: 13 - t A3), each previously trained for one of the defined targets. A ranking\\nprocedure is used to determine the target object. The pixel coordinates ofthe target\\nin the segmented image are mapped by the PSOM to world coordinates (P3: A3 - t\\ncartesian target position). The robot R now moves to above these world coordinates,\\nmoves vertically down, grabs whatever is there, and moves upward again. Now, the\\nsystem evaluates a second pointing gesture that specifies the place where to place\\nthe object. This time, the world coordinates calculated on the basis of the pointing\\ndirection from network D and the cartesian hand location from PSOM PI serve\\ndirectly as target location for the robot.\\nFor our processing we must map corresponding pixels in the stereo images to cartesian world coordinates. For these transformations, training data was generated\\nwith aid of the robot on a precise sampling grid. We automatically extract the\\npixel coordinates of a LED at the tip of the robot manipulator from both images.\\nThe seven-dimensional feature vector serves as training input for an PSOM network [11]. By virtue of its capability to represent a transformation in a symmetric,\\n\\\"multiway\\\" -fashion, this offers the additional benefit that both the camera-to-world\\nmapping and its inverse can be obtained with a single network trained only once on\\na data set of 27 calibration positions of the robot. A detailed description for such\\na procedure can be found in [13].\\n\\n3\\n\\nResults\\n\\n3.1 System performance\\nThe accuracy of the current system allows to estimate the pointing target to an\\naccuracy of 1 ? 0.4 cm (average over N = 7 objects at randomly chosen locations\\nin the workspace) in a workspace area of 50x50 cm. In our current environment,\\nthis is sufficient to pick and place any of the seven defined target objects at any\\nlocation in the workspace. This accuracy can only be achieved if we use the object\\nrecognition module described in sec. 2. The output of the pointing direction module\\napproximates the target location with an considerably lower accuracy of 3.6? 1.6 cm.\\n3.2 Image segmentation\\nThe problem to evaluate these preprocessing steps has been discussed previously [7],\\nespecially the relation of specifity and sensitivity of the network for the given task.\\nAs the pointing recognition is based on a subframe centered on the hand center, it\\nis very sensitive to deviations from this center so that a good localization accuracy\\n\\n\\fVisual Gesture-based Robot Guidance with a Modular Neural System\\n\\n907\\n\\nis even more important than the classification rate. The localization accuracy is\\ncalculated by measuring the pixel distance between the centers determined manually on the original image and as the center of mass in the image obtained after\\napplication of the neural network. Table 1 provides quantitative results.\\nOn the whole) the two-step cascade of LLM networks yields for 399 out of 400 images\\nan activity image precisely centered on the human hand. Only in one image) the\\nfirst LLM net missed the hand completely) due to a second hand in the image that\\ncould be clearly seen in this view. This image was excluded from further processing\\nand from the evaluation of the localization accuracy.\\n\\nPerson A\\nPerson H\\n\\nCamera A\\nPixel deviatIOn\\nNRMSE\\n0.8 ? 1.2\\n0.03 ? 0.06\\n1.3 ? 1.4\\n0.06 ? 0.11\\n\\nCamera B\\nPixel deViatIOn\\nNRMSE\\n0.8 ? 2.2\\n0.03 ? 0.09\\n2.2 ? 2.8\\n0.11 ? 0.21\\n\\nTable 1: Estimation error of the hand localization on the test set. Absolute error in pixels\\nand normalized error for both persons and both camera images.\\n\\n3.3 Recognition performance\\nOne major problem in recognizing human pointing gestures is the variability of these\\ngestures and their measurement for the acquisition of reliable training information.\\nDifferent persons follow different strategies where and how to point (fig. 2 (center)\\nand (right?. Therefore) we calculate this information indirectly. The person is\\ntold to point at a certain grid position with known world coordinates. From the\\ncamera images we extract the pixel positions of the hand center and map them to\\nworld coordinates using the PSOM net (PI in fig . 1). Given these coordinates the\\nangles of the intended pointing vector with the basis vectors of the world coordinate\\nsystem can be calculated trigonometrically. These angles form the target vector for\\nthe supervised training of a LLM network (D in fig. 1).\\n\\nAfter training) the output of the net is used to calculate the point where the pointing\\nvector intersects the table surface. For evaluation of the network performance we\\nmeasure the Euclidian distance between this point and the actual grid point where\\nthe person intended to point at. Fig. 3 (left) shows the mean euclidean error MEE\\nof the estimated target position as a function of the number of learning steps. The\\nerror on the training set can be considerably reduced) whereas on the test set the\\nimprovement stagnates after some 500 training steps. If we perform even more\\ntraining steps the performance might actually suffer from overfitting. The graph\\ncompares training and test results achieved on images obtained by two different\\nways of determining the hand center. The \\\"manual\\\" curves show the performance\\nthat can be achieved if the Gabor masks are manually centered on the hand. For\\nthe \\\"neuronal)) curves) the center of mass calculated in the fine-segmented and postprocessed subframe was used. This allows us to study the influence of the error of\\nthe segmentation and localization steps on the pointing recognition. This influence\\nis rather small. The MEE increases from 17 mm for the optimal method to 19 mm\\nfor the neural method) which is hardly visible in practice.\\nThe curves in fig. 3 (center) are obtained if we apply the networks to images of\\nanother person. The MEE is considerably larger but a detailed analysis' shows\\nthat part of this deviation is due to systematic differences in the pointing strategy\\nas shown in fig. 2 (right). Over a wide range, the number of nodes used for the\\nLLM network has only minor influence on the performance. While obviously the\\nperformance on the training set can be arbitrarily improved by spending more nodes,\\nthe differences in the MEE on the test set are negligible in a range of 5 to 15 nodes.\\nUsing more nodes is problematic as the training data consists of 50 examples only.\\nIf not indicated otherwise) we use LLM networks with 10 nodes. Further results)\\n\\n\\f908\\n\\nE. LIITMANN. A. DREES. H. RIITER\\n\\nFig. 2: The table grid points can be reconstructed according to the network output. The\\ntarget grid is dotted . Reconstruction of training grid (left) and test grid (center) for one\\nperson, and of the test grid for another person (right).\\nMEB on test oet of unknown perron\\n\\nMER\\n30\\n\\n20\\n\\ne?\\n\\nI~\\n\\n10\\n\\n~\\n\\n---- ~--.---\\n\\n~\\n~-\\n\\n0\\n\\nn\\n\\nm..... aI,trainneuronal, train manual, test -\\n\\n:l~\\n\\n100\\n\\n250\\n\\nsao\\n\\n1000 2SOO SOOO\\n\\ntrain.., itHabonr\\n\\ne\\n?\\n\\n70\\n68\\n66\\n64\\n62\\n60\\n58\\n56\\n\\n4\\n\\n-~.\\n\\n100\\n\\n:l~\\n\\nsao\\n\\n1000\\n\\n2SOO SOOO\\n\\nFig. 3: The euclidean error of\\nestimated target point calculated using the network output depends on the preprocessing (left), and the person\\n(center).\\n\\ntrairq IteratioN\\n\\ncomparing the pointing recognition based on only one of the camera images, indicate\\nthat the method works better if the camera takes a lateral view rather than a frontal\\nview . All evaluations were done for both persons. The performance was always very\\nsimilar.\\n\\n4\\n\\nDiscussion\\n\\nWhile we begin to understand many properties of neural networks at the single\\nnetwork level, our insight into principled ways of how to build neural systems is\\nstill rather limited . Due to the complexity of this task, theoretical progress is\\n(and probably will continue to be) very slow. What we can do in the mean time,\\nhowever, is to experiment with different design strategies for neural systems and\\ntry to \\\"evolve\\\" useful approaches by carefully chosen case studies.\\nThe current work is an effort along these lines. It is focused on a challenging,\\npractically important vision task with a number of generic features that are shared\\nwith vision tasks for which biological vision systems were evolved.\\nOne important issue is how to achieve robustness at the different processing levels\\nof the system. There are only very limited possibilities to study this issue in simulations, since practically nothing is known about the statistical properties of the\\nvarious sources of error that occur when dealing with real world data. Thus, a real\\nimplementation that works with actual data is practically the only way to study\\nthe robustness issue in a realistic fashion. Therefore, the demonstrated integration\\nof several functional modules that we had developed previously in more restricted\\nsettings [7, 6] was a non-trivial test of the feasability of having these functions\\ncooperate in a larger, modular system. It also gives confidence that the scaling\\nproblem can be dealt with successfully if we apply modular neural nets.\\nA related and equally important issue was the use of a processing strategy in which\\nearlier processing stages incrementally restrict the search space for the subsequent\\nstages. Thus, the responsibility for achieving the goal is not centralized in any single\\nmodule and subsequent modules have always the chance to compensate for limited\\nerrors of earlier stages. This appears to be a generally useful strategy for achieving\\n\\n\\fVisual Gesture-based Robot Guidance with a Modular Neural System\\n\\n909\\n\\nrobustness and for cutting computational costs that is related to the use of \\\"focal\\nattention\\\" , which is clearly an important element of many biological vision systems.\\nA third important point is the extensive use of learning to build the essential constituent functions of the system from data examples. We are not yet able to train\\nthe assembled system as a whole. Instead, different modules are trained separately\\nand are integrated only later. Still, the experience gained with assembling a complex system via this \\\"engineering-type\\\" of approach will be extremely valuable for\\ngradually developing the capability of crafting larger functional building blocks by\\nlearning methods.\\nWe conclude that carefully designed experiments with modular neural systems that\\nare based on the use of real world data and that focus on similar tasks for which\\nalso biological neural systems were evolved can make a significant contribution in\\ntackling the challenge that lies ahead of us: to develop a reliable technology for the\\nconstruction of large-scale artificial neural systems that can solve complex tasks in\\nreal world environments.\\nAcknowledgements\\nWe want to thank Th. Wengerek (robot control), J. Walter (PSOM implementation), and\\nP. Ziemeck (image acquisition software). This work was supported by BMFT Grant No.\\nITN9104AO.\\n\\nReferences\\n[1] T. J. Darell and A. P. Pentland. Classifying hand gestures with a view-based distributed representation. In J . D. Cowan, G. Tesauro, and J. Alspector, editors, Neural\\nInformation Processing Systems 6, pages 945-952. Morgan Kaufman, 1994.\\n[2] J. Davis and M. Shah. Recognizing hand gestures. In J.-O. Eklundh, editor, Computer\\nVision - ECCV '94, volume 800 of Lecture Notes in Computer Science, pages 331340. Springer-Verlag, Berlin Heidelberg New York, 1994.\\n[3] R.A. Jacobs, M.1. Jordan, S.J. Nowlan, and G.E. Hinton. Adaptive mixtures of local\\nexperts. Neural Computation, 3:79- 87, 1991.\\n[4] M.1. Jordan and R.A. Jacobs. Hierarchical mixtures of experts and the EM algorithm.\\nNeural Computation, 6(2):181-214, 1994.\\n[5] F. Kummert, E. Littmann, A. Meyering, S. Posch, H. Ritter, and G. Sagerer. A\\nhybrid approach to signal interpretation using neural and semantic networks. In\\nMustererkennung 1993, pages 245-252. Springer, 1993.\\n[6] E. Littmann, A. Drees, and H. Ritter. Neural recognition of human pointing gestures\\nin real images. Submitted to Neural Processing Letters, 1996.\\n[7] E. Littmann and H. Ritter. Neural and statistical methods for adaptive color segmentation - a comparison. In G. Sagerer, S. Posch, and F. Kummert, editors,\\nMustererkennung 1995, pages 84-93. Springer-Verlag, Heidelberg, 1995.\\n[8] C. Maggioni. A novel device for using the hand as a human-computer interface. In\\nProceedings HC1'93 - Human Control Interface, Loughborough, Great Britain, 1993.\\n[9] A. Meyering and H. Ritter. Learning 3D shape perception with local linear maps. In\\nProc. of the lJCNN, volume IV, pages 432-436, Baltimore, MD, 1992.\\n[10] Steven J. Nowlan and John C. Platt. A convolutional neural network hand tracker.\\nIn Neural Information Processing Systems 7. Morgan Kaufman Publishers, 1995.\\n[11] H. Ritter. Parametrized self-organizing maps for vision learning tasks. In P. Morasso,\\neditor, ICANN '94. Springer-Verlag, Berlin Heidelberg New York, 1994.\\n[12] K. Viiiina.nen and K. Bohm. Gesture driven interaction as a human factor in virtual\\nenvironments - an approach with neural networks. In R. Earnshaw, M. Gigante, and\\nH. Jones, editors, Virtual reality systems, pages 93-106. Academic Press, 1993.\\n[13] J. Walter and H. Ritter. Rapid learning with parametrized self-organizing maps.\\nNeural Computing, 1995. Submitted.\\n[14] T. G. Zimmermann, J. Lanier, C. Blanchard, S. Bryson, and Y. Harvill. A hand\\ngesture interface device. In Proc. CHI+GI, pages 189-192, 1987.\\n\\n\\f\",\n          \"Iterative Non-linear Dimensionality Reduction by\\nManifold Sculpting\\n\\nMike Gashler, Dan Ventura, and Tony Martinez ?\\nBrigham Young University\\nProvo, UT 84604\\n\\nAbstract\\nMany algorithms have been recently developed for reducing dimensionality by\\nprojecting data onto an intrinsic non-linear manifold. Unfortunately, existing algorithms often lose significant precision in this transformation. Manifold Sculpting\\nis a new algorithm that iteratively reduces dimensionality by simulating surface\\ntension in local neighborhoods. We present several experiments that show Manifold Sculpting yields more accurate results than existing algorithms with both\\ngenerated and natural data-sets. Manifold Sculpting is also able to benefit from\\nboth prior dimensionality reduction efforts.\\n\\n1\\n\\nIntroduction\\n\\nDimensionality reduction is a two-step process: 1) Transform the data so that more information\\nwill survive the projection, and 2) project the data into fewer dimensions. The more relationships\\nbetween data points that the transformation step is required to preserve, the less flexibility it will have\\nto position the points in a manner that will cause information to survive the projection step. Due\\nto this inverse relationship, dimensionality reduction algorithms must seek a balance that preserves\\ninformation in the transformation without losing it in the projection. The key to finding the right\\nbalance is to identify where the majority of the information lies.\\nNonlinear dimensionality reduction (NLDR) algorithms seek this balance by assuming that the relationships between neighboring points contain more informational content than the relationships\\nbetween distant points. Although non-linear transformations have more potential than do linear\\ntransformations to lose information in the structure of the data, they also have more potential to\\nposition the data to cause more information to survive the projection. In this process, NLDR algorithms expose patterns and structures of lower dimensionality (manifolds) that exist in the original\\ndata. NLDR algorithms, or manifold learning algorithms, have potential to make the high-level\\nconcepts embedded in multidimensional data accessible to both humans and machines.\\nThis paper introduces a new algorithm for manifold learning called Manifold Sculpting, which discovers manifolds through a process of progressive refinement. Experiments show that it yields\\nmore accurate results than other algorithms in many cases. Additionally, it can be used as a postprocessing step to enhance the transformation of other manifold learning algorithms.\\n\\n2\\n\\nRelated Work\\n\\nMany algorithms have been developed for performing non-linear dimensionality reduction. Recent\\nworks include Isomap [1], which solves for an isometric embedding of data into fewer dimensions\\nwith an algebraic technique. Unfortunately, it is somewhat computationally expensive as it requires\\nsolving for the eigenvectors of a large dense matrix, and has difficulty with poorly sampled areas of\\n?\\n\\nmikegashler@gmail.com, ventura@cs.byu.edu, martinez@cs.byu.edu\\n\\n1\\n\\n\\fFigure 1: Comparison of several manifold learners on a Swiss Roll manifold. Color is used to\\nindicate how points in the results correspond to points on the manifold. Isomap and L-Isomap have\\ntrouble with sampling holes. LLE has trouble with changes in sample density.\\n\\nthe manifold. (See Figure 1.A.) Locally Linear Embedding (LLE) [2] is able to perform a similar\\ncomputation using a sparse matrix by using a metric that measures only relationships between vectors in local neighborhoods. Unfortunately it produces distorted results when the sample density is\\nnon-uniform. (See Figure 1.B.) An improvement to the Isomap algorithm was later proposed that\\nuses landmarks to reduce the amount of necessary computation [3]. (See Figure 1.C.) Many other\\nNLDR algorithms have been proposed, including Kernel Principle Component Analysis [4], Laplacian Eigenmaps [5], Manifold Charting [6], Manifold Parzen Windows [7], Hessian LLE [8], and\\nothers [9, 10, 11]. Hessian LLE preserves the manifold structure better than the other algorithms but\\nis, unfortunately, computationally expensive. (See Figure 1.D.).\\nIn contrast with these algorithms, Manifold Sculpting is robust to sampling issues and still produces\\nvery accurate results. This algorithm iteratively transforms data by balancing two opposing heuristics, one that scales information out of unwanted dimensions, and one that preserves local structure\\nin the data. Experimental results show that this technique preserves information into fewer dimensions with more accuracy than existing manifold learning algorithms. (See Figure 1.E.)\\n\\n3\\n\\nThe Algorithm\\n\\nAn overview of the Manifold Sculpting algorithm is given in Figure 2a.\\n\\nFigure 2: ? and ? define the relationships that Manifold Sculpting attempts to preserve.\\n\\n2\\n\\n\\fStep 1: Find the k nearest neighbors of each point. For each data point pi in P (where P is the set\\nof all data points represented as vectors in Rn ), find the k-nearest neighbors Ni (such that nij ? Ni\\nis the j th neighbor of point pi ).\\nStep 2: Compute relationships between neighbors. For each j (where 0 < j ? k) compute the\\nEuclidean distance ?ij between pi and each nij ? Ni . Also compute the angle ?ij formed by the\\ntwo line segments (pi to nij ) and (nij to mij ), where mij is the most colinear neighbor of nij with\\npi . (See Figure 2b.) The most colinear neighbor is the neighbor point that forms the angle closest\\nto ?. The values of ? and ? are the relationships that the algorithm will attempt to preserve during\\ntransformation. The global average distance between all the neighbors of all points ?ave is also\\ncomputed.\\nStep 3: Optionally preprocess the data. The data may optionally be preprocessed with the transformation step of Principle Component Analysis (PCA), or another efficient algorithm. Manifold\\nSculpting will work without this step; however, preprocessing can result in significantly faster convergence. To the extent that there is a linear component in the manifold, PCA will move the information in the data into as few dimensions as possible, thus leaving less work to be done in step 4\\n(which handles the non-linear component). This step is performed by computing the first |Dpres |\\nprinciple components of the data (where Dpres is the set of dimensions that will be preserved in\\nthe projection), and rotating the dimensional axes to align with these principle components. (An\\nefficient algorithm for computing principle components is presented in [12].)\\nStep 4: Transform the data. The data is iteratively transformed until some stopping criterion has\\nbeen met. One effective technique is to stop when the sum change of all points during the current\\niteration falls below a threshold. The best stopping criteria depend on the desired quality of results ?\\nif precision is important, the algorithm may iterate longer; if speed is important it may stop earlier.\\nStep 4a: Scale values. All the values in Dscal (The set of dimensions that will be eliminated by the\\nprojection) are scaled by a constant factor ?, where 0 < ? < 1 (? = 0.99 was used in this paper).\\nOver time, the values in Dscal will converge to 0. When Dscal is dropped by the projection (step 5),\\nthere will be very little informational content left in these dimensions.\\nStep 4b: Restore original relationships. For each pi ? P , the values in Dpres are adjusted to\\nrecover the relationships that are distorted by scaling. Intuitively, this step simulates tension on the\\nmanifold surface. A heuristic error value is used to evaluate the current relationships among data\\npoints relative to the original relationships:\\n\\u0012\\n\\u00132 \\u0012\\n\\u00132 !\\nk\\nX\\n?ij ? ?ij0\\n?ij ? ?ij0\\nwij\\n\\u000fpi =\\n+\\n(1)\\n2?ave\\n?\\nj=0\\nwhere ?ij is the current distance to nij , ?ij0 is the original distance to nij measured in step 2, ?ij\\nis the current angle, and ?ij0 is the original angle measured in step 2. The denominator values\\nwere chosen as normalizing factors because the value of the angle term can range from 0 to ?, and\\nthe value of the distance term will tend to have a mean of about ?ave with some variance in both\\ndirections. We adjust the values in Dpres for each point to minimize this heuristic error value.\\nThe order in which points are adjusted has some impact on the rate of convergence. Best results were\\nobtained by employing a breadth-first neighborhood graph traversal from a randomly selected point.\\n(A new starting point is randomly selected for each iteration.) Intuitively this may be analogous to\\nthe manner in which a person smoothes a crumpled piece of paper by starting at an arbitrary point\\nand smoothing outward. To further speed convergence, higher weight, wij , is given to the component\\nof the error contributed by neighbors that have already been adjusted in the current iteration. For all\\nof our experiments, we use wij = 1 if ni has not yet been adjusted in this iteration, and wij = 10,\\nif nij has been adjusted in this iteration.\\nUnfortunately the equation for the true gradient of the error surface defined by this heuristic is\\ncomplex, and is in O(|D|3 ). We therefore use the simple hill-climbing technique of adjusting in\\neach dimension in the direction that yields improvement.\\nSince the error surface is not necessarily convex, the algorithm may potentially converge to local\\nminima. At least three factors, however, mitigate this risk: First, the PCA pre-processing step often\\ntends to move the whole system to a state somewhat close to the global minimum. Even if a local\\n3\\n\\n\\fFigure 3: The mean squared error of four algorithms with a Swiss Roll manifold using a varying\\nnumber of neighbors k. When k > 57, neighbor paths cut across the manifold. Isomap is more\\nrobust to this problem than other algorithms, but HLLE and Manifold Sculpting still yield better\\nresults. Results are shown on a logarithmic scale.\\nminimum exists so close to the globally optimal state, it may have a sufficiently small error as to be\\nacceptable. Second, every point has a unique error surface. Even if one point becomes temporarily\\nstuck in a local minimum, its neighbors are likely to pull it out, or change the topology of its error\\nsurface when their values are adjusted. Very particular conditions are necessary for every point to\\nsimultaneously find a local minimum. Third, by gradually scaling the values in Dscaled (instead of\\ndirectly setting them to 0), the system always remains in a state very close to the current globally\\noptimal state. As long as it stays close to the current optimal state, it is unlikely for the error\\nsurface to change in a manner that permanently separates it from being able to reach the globally\\noptimal state. (This is why all the dimensions need to be preserved in the PCA pre-processing step.)\\nAnd perhaps most significantly, our experiments show that Manifold Sculpting generally tends to\\nconverge to very good results.\\nStep 5: Project the data. At this point Dscal contains only values that are very close to zero. The\\ndata is projected by simply dropping these dimensions from the representation.\\n\\n4\\n\\nEmpirical Results\\n\\nFigure 1 shows that Manifold Sculpting appears visually to produce results of higher quality than\\nLLE and Isomap with the Swiss Roll manifold, a common visual test for manifold learning algorithms. Quantitative analysis shows that it also yields better results than HLLE. Since the actual\\nstructure of this manifold is known prior to using any manifold learner, we can use this prior information to quantitatively measure the accuracy of each algorithm.\\n4.1\\n\\nVarying number of neighbors.\\n\\nWe define a Swiss Roll in 3D space with n points (xi , yi , zi ) for each 0 ? i < n, such that xi =\\nt sin(t), yi is a random number ?6 ? yi < 6, and zi = t cos(t), ?where t = 8i/n + 2. In 2D\\n?1\\nt2 +1\\nand vi = yi .\\nmanifold coordinates, the point is (ui , vi ), such that ui = sinh (t)+t\\n2\\nWe created a Swiss Roll with 2000 data points and reduced the dimensionality to 2 with each of four\\nalgorithms. Next we tested how well these results align with the expected values by measuring the\\nmean squared distance from each point to its expected value. (See Figure 3.) We rotated, scaled,\\nand translated the values as required to obtain the minimum possible error measurement for each\\nalgorithm. These results are consistent with a qualitative assessment of Figure 1. Results are shown\\nwith a varying number of neighbors k. In this example, when k = 57, local neighborhoods begin\\nto cut across the manifold. Isomap is more robust to this problem than other algorithms, but HLLE\\nand Manifold Sculpting still yield better results.\\n4\\n\\n\\fFigure 4: The mean squared error of points from an S-Curve manifold for four algorithms with a\\nvarying number of data points. Manifold Sculpting shows a trend of increasing accuracy with an\\nincreasing number of points. This experiment was performed with 20 neighbors. Results are shown\\non a logarithmic scale.\\n4.2\\n\\nVarying sample densities.\\n\\nA similar experiment was performed with an S-Curve manifold. We defined the S-Curve points in\\n3D space with n points (xi , yi , zi ) for each 0 ? i < n, such that xi = t, yi = sin(t), and zi is\\na random number 0 ? zi < 2, where t = (2.2i?0.1)?\\n. In 2D manifold coordinates, the point is\\nn\\nZ t \\u0010p\\n\\u0011\\ncos2 (w) + 1 dw and vi = yi .\\n(ui , vi ), such that ui =\\n0\\n\\nFigure 4 shows the mean squared error of the transformed points from their expected values using\\nthe same regression technique described for the experiment with the Swiss Roll problem. We varied\\nthe sampling density to show how this affects each algorithm. A trend can be observed in this data\\nthat as the number of sample points increases, the quality of results from Manifold Sculpting also\\nincreases. This trend does not appear in the results from other algorithms.\\nOne drawback to the Manifold Sculpting algorithm is that convergence may take longer when the\\nvalue for k is too small. This experiment was also performed with 6 neighbors, but Manifold Sculpting did not always converge within a reasonable time when so few neighbors were used. The other\\nthree algorithms do not have this limitation, but the quality of their results still tend to be poor when\\nvery few neighbors are used.\\n4.3\\n\\nEntwined spirals manifold.\\n\\nA test was also performed with an Entwined Spirals manifold. In this case, Isomap was able to\\nproduce better results than Manifold Sculpting (see Figure 5), even though Isomap yielded the worst\\naccuracy in previous problems. This can be attributed to the nature of the Isomap algorithm. In cases\\nwhere the manifold has an intrinsic dimensionality of exactly 1, a path from neighbor to neighbor\\nprovides an accurate estimate of isolinear distance. Thus an algorithm that seeks to globally optimize isolinear distances will be less susceptible to the noise from cutting across local corners. When\\nthe intrinsic dimensionality is higher than 1, however, paths that follow from neighbor to neighbor\\nproduce a zig-zag pattern that introduces excessive noise into the isolinear distance measurement. In\\nthese cases, preserving local neighborhood relationships with precision yields better overall results\\nthan globally optimizing an error-prone metric. Consistent with this intuition, Isomap is the closest\\ncompetitor to Manifold Sculpting in other experiments that involved a manifold with a single intrinsic dimension, and yields the poorest results of the four algorithms when the intrinsic dimensionality\\nis larger than one.\\n5\\n\\n\\fFigure 5: Mean squared error for four algorithms with an Entwined Spirals manifold.\\n4.4\\n\\nImage-based manifolds.\\n\\nThe accuracy of Manifold Sculpting is not limited to generated manifolds in three dimensional\\nspace. Unfortunately, the manifold structure represented by most real-world problems is not known\\na priori. The accuracy of a manifold learner, however, can still be estimated when the problem\\ninvolves a video sequence by simply counting the percentage of frames that are sorted into the same\\norder as the video sequence. Figure 6 shows several frames from a video sequence of a person\\nturning his head while gradually smiling. Each image was encoded as a vector of 1, 634 pixel\\nintensity values. This data was then reduced to a single dimension. (Results are shown on three\\nseparate lines in order to fit the page.) The one preserved dimension could then characterize each\\nframe according to the high-level concepts that were previously encoded in many dimensions. The\\ndot below each image corresponds to the single-dimensional value in the preserved dimension for\\nthat image. In this case, the ordering of every frame was consistent with the video sequence.\\n4.5\\n\\nControlled manifold topologies.\\n\\nFigure 7 shows a comparison of results obtained from a manifold generated by translating an image\\nover a background of random noise. Nine of the 400 input images are shown as a sample, and\\nresults with each algorithm are shown as a mesh. Each vertex is placed at a position corresponding\\nto the two values obtained from one of the 400 images. For increased visibility of the inherent\\nstructure, the vertexes are connected with their nearest input space neighbors. Because two variables\\n(horizontal position and vertical position) were used to generate the dataset, this data creates a\\nmanifold with an intrinsic dimensionality of two in a space with an extrinsic dimensionality of\\n2,401 (the total number of pixels in each image). Because the background is random, the average\\ndistance between neighboring points in the input space is uniform, so the ideal result is known to\\nbe a square. The distortions produced by Manifold Sculpting tend to be local in nature, while the\\ndistortions produced by other algorithms tend to be more global. Note that the points are spread\\nnearly uniformly across the manifold in the results from Manifold Sculpting. This explains why the\\nresults from Manifold Sculpting tend to fit the ideal results with much lower total error (as shown in\\n\\nFigure 6: Images of a face reduced by Manifold Sculpting into a single dimension. The values are\\nare shown here on three wrapped lines in order to fit the page. The original image is shown above\\neach point.\\n6\\n\\n\\fFigure 7: A comparison of results with a manifold generated by translating an image over a background of noise. Manifold Sculpting tends to produce less global distortion, while other algorithms\\ntend to produce less local distortion. Each point represents an image. This experiment was done\\nin each case with 8 neighbors. (LLE fails to yield results with these parameters, but [13] reports a\\nsimilar experiment in which LLE produces results. In that case, as with Isomap and HLLE as shown\\nhere, distortion is clearly visible near the edges.)\\n\\nFigure 3 and Figure 4). Perhaps more significantly, it also tends to keep the intrinsic variables in the\\ndataset more linearly separable. This is particularly important when the dimensionality reduction is\\nused as a pre-processing step for a supervised learning algorithm.\\nWe created four video sequences designed to show various types of manifold topologies and measured the accuracy of each manifold learning algorithm. These results (and sample frames from each\\nvideo) are shown in Figure 8. The first video shows a rotating stuffed animal. Since the background\\npixels remain nearly constant while the pixels on the rotating object change in value, the manifold\\ncorresponding to the vector encoding of this video will contain both smooth and changing areas.\\nThe second video was made by moving a camera down a hallway. This produces a manifold with a\\ncontinuous range of variability, since pixels near the center of the frame change slowly while pixels\\nnear the edges change rapidly. The third video pans across a scene. Unlike the video of the rotating\\nstuffed animal, there are no background pixels that remain constant. The last video shows another\\nrotating stuffed animal. Unlike the first video, however, the high-contrast texture of the object used\\nin this video results in a topology with much more variation. As the black spots shift across the\\npixels, a manifold is created that swings wildly in the respective dimensions. Due to the large hills\\nand valleys in the topology of this manifold, the nearest neighbors of a frame frequently create paths\\nthat cut across the manifold. In all four cases, Manifold Sculpting produced results competitive\\nwith Isomap, which does particularly well with manifolds that have an intrinsic dimensionality of\\n\\nFigure 8: Four video sequences were created with varying properties in the corresponding manfolds.\\nDimensionality was reduced to one with each of four manifold learning algorithms. The percentage\\nof frames that were correctly ordered by each algorithm is shown.\\n\\n7\\n\\n\\fone, but Manifold Sculpting is not limited by the intrinsic dimensionality as shown in the previous\\nexperiments.\\n\\n5\\n\\nDiscussion\\n\\nThe experiments tested in this paper show that Manifold Sculpting yields more accurate results\\nthan other well-known manifold learning algorithms. Manifold Sculpting is robust to holes in the\\nsampled area. Manifold Sculpting is more accurate than other algorithms when the manifold is\\nsparsely sampled, and the gap is even wider with higher sampling densities. Manifold Sculpting\\nhas difficulty when the selected number of neighbors is too small but consistently outperforms other\\nalgorithms when it is larger.\\nDue to the iterative nature of Manifold Sculpting, it?s difficult to produce a valid complexity analysis.\\nConsequently, we measured the scalability of Manifold Sculpting empirically and compared it with\\nthat of HLLE, L-Isomap, and LLE. Due to space constraints these results are not included here, but\\nthey indicate that Manifold Sculpting scales better than the other algorithms when when the number\\nof data points is much larger than the number of input dimensions.\\nManifold Sculpting benefits significantly when the data is pre-processed with the transformation step of PCA. The transformation step of any algorithm may be used in place of this step.\\nCurrent research seeks to identify which algorithms work best with Manifold Sculpting to efficiently produce high quality results. (An implementation of Manifold Sculpting is included at\\nhttp://waffles.sourceforge.net.)\\n\\nReferences\\n[1] Joshua B. Tenenbaum, Vin de Silva, and John C. Langford. A global geometric framework for\\nnonlinear dimensionality reduction. Science, 290:2319?2323, 2000.\\n[2] Sam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally linear\\nembedding. Science, 290:2323?2326, 2000.\\n[3] Vin de Silva and Joshua B. Tenenbaum. Global versus local methods in nonlinear dimensionality reduction. In NIPS, pages 705?712, 2002.\\n[4] Bernhard Sch?olkopf, Alexander J. Smola, and Klaus-Robert M?uller. Kernel principal component analysis. Advances in kernel methods: support vector learning, pages 327?352, 1999.\\n[5] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In Advances in Neural Information Processing Systems, 14, pages 585?\\n591, 2001.\\n[6] Matthew Brand. Charting a manifold. In Advances in Neural Information Processing Systems,\\n15, pages 961?968. MIT Press, Cambridge, MA, 2003.\\n[7] Pascal Vincent and Yoshua Bengio. Manifold parzen windows. In Advances in Neural Information Processing Systems 15, pages 825?832. MIT Press, Cambridge, MA, 2003.\\n[8] D. Donoho and C. Grimes. Hessian eigenmaps: locally linear embedding techniques for high\\ndimensional data. Proc. of National Academy of Sciences, 100(10):5591?5596, 2003.\\n[9] Yoshua Bengio and Martin Monperrus. Non-local manifold tangent learning. In Advances\\nin Neural Information Processing Systems 17, pages 129?136. MIT Press, Cambridge, MA,\\n2005.\\n[10] Elizaveta Levina and Peter J. Bickel. Maximum likelihood estimation of intrinsic dimension.\\nIn NIPS, 2004.\\n[11] Zhenyue Zhang and Hongyuan Zha. A domain decomposition method for fast manifold learning. In Y. Weiss, B. Sch?olkopf, and J. Platt, editors, Advances in Neural Information Processing\\nSystems 18. MIT Press, Cambridge, MA, 2006.\\n[12] Sam Roweis. Em algorithms for PCA and SPCA. In Michael I. Jordan, Michael J. Kearns, and\\nSara A. Solla, editors, Advances in Neural Information Processing Systems, volume 10, 1998.\\n[13] Lawrence K. Saul and Sam T. Roweis. Think globally, fit locally: Unsupervised learning of\\nlow dimensional manifolds. Journal of Machine Learning Research, 4:119?155, 2003.\\n\\n8\\n\\n\\f\",\n          \"Diffeomorphic Dimensionality Reduction\\n\\nChristian Walder and Bernhard Sch?olkopf\\nMax Planck Institute for Biological Cybernetics\\n72076 T?ubingen, Germany\\nfirst.last@tuebingen.mpg.de\\n\\nAbstract\\nThis paper introduces a new approach to constructing meaningful lower dimensional representations of sets of data points. We argue that constraining the mapping between the high and low dimensional spaces to be a diffeomorphism is a\\nnatural way of ensuring that pairwise distances are approximately preserved. Accordingly we develop an algorithm which diffeomorphically maps the data near to\\na lower dimensional subspace and then projects onto that subspace. The problem\\nof solving for the mapping is transformed into one of solving for an Eulerian flow\\nfield which we compute using ideas from kernel methods. We demonstrate the\\nefficacy of our approach on various real world data sets.\\n\\n1\\n\\nIntroduction\\n\\nThe problem of visualizing high dimensional data often arises in the context of exploratory data\\nanalysis. For many real world data sets this is a challenging task, as the spaces in which the data\\nlie are often too high dimensional to be visualized directly. If the data themselves lie on a lower\\ndimensional subspace however, dimensionality reduction techniques may be employed, which aim\\nto meaningfully represent the data as elements of this lower dimensional subspace.\\nThe earliest approaches to dimensionality reduction are the linear methods known as principal components analysis (PCA) and factor analysis (Duda et al., 2000). More recently however, the majority of research has focussed on non-linear methods, in order to overcome the limitations of linear\\napproaches?for an overview and numerical comparison see e.g. (Venna, 2007; van der Maaten\\net al., 2008), respectively. In an effort to better understand the numerous methods which have been\\nproposed, various categorizations have been proposed. In the present case, it is pertinent to make\\nthe distinction between methods which focus on properties of the mapping to the lower dimensional\\nspace, and methods which focus on properties of the mapped data, in that space. A canonical example of the latter is multidimensional scaling (MDS), which in its basic form finds the minimizer\\nwith respect to y1 , y2 , . . . , ym of (Cox & Cox, 1994)\\nm\\nX\\n\\n2\\n\\n(kxi ? xj k ? kyi ? yj k) ,\\n\\n(1)\\n\\ni,j=1\\n\\nwhere here, as throughout the paper, the xi ? Ra are input or high dimensional points, and the\\nyi ? Rb are output or low dimensional points, so that b < a. Note that the above term is a\\nfunction only of the input points and the corresponding mapped points, and is designed to preserve\\nthe pairwise distances of the data set.\\nThe methods which focus on the mapping itself (from the higher to the lower dimensional space,\\nwhich we refer to as the downward mapping, or the upward mapping which is the converse) are less\\ncommon, and form a category into which the present work falls. Both auto-encoders (DeMers &\\nCottrell, 1993) and the Gaussian process latent variable model (GP-LVM) (Lawrence, 2004) also\\nfall into this category, but we focus on the latter as it provides an appropriate transition into the\\n1\\n\\n\\fmain part the paper. The GP-LVM places a Gaussian process (GP) prior over each high dimensional component of the upward mapping, and optimizes with respect to the set of low dimensional\\npoints?which can be thought of as hyper-parameters of the model?the likelihood of the high dimensional points. Hence the GP-LVM constructs a regular (in the sense of regularization, i.e. likely\\nunder the GP prior) upward mapping. By doing so, the model guarantees that nearby points in\\nthe low dimensional space should be mapped to nearby points in the high dimensional space?an\\nintuitive idea for dimensionality reduction which is also present in the MDS objective (1), above.\\nThe converse is not guaranteed in the original GP-LVM however, and this has lead to the more recent development of the so-called back-constrained GP-LVM (Lawrence & Candela, 2006), which\\nessentially places an additional GP prior over the downward mapping. By guaranteeing in this way\\nthat (the modes of the posterior distributions over) both the upward and downward mappings are\\nregular, the back constrained GP-LVM induces something reminiscent of a diffeomorphic mapping\\nbetween the two spaces. This leads us to the present work, in which we derive our new algorithm,\\nDiffeomap, by explicitly casting the dimensionality reduction problem as one of constructing a diffeomorphic mapping between the low dimensional space and the subspace of the high dimensional\\nspace on which the data lie.\\n\\n2\\n\\nDiffeomorphic Mappings and their Practical Construction\\n\\nIn this paper we use the following definition:\\nDefinition 2.1. Let U and V be open subsets of Ra and Rb , respectively. The mapping F : U ? V\\nis said to be a diffeomorphism if it is bijective (i.e. one to one), smooth (i.e. belonging to C ? ), and\\nhas a smooth inverse map F ?1 .\\nWe note in passing the connection between this definition, our discussion of the GP-LVM, and dimensionality reduction. The GP-LVM constructs a regular upward mapping (analogous to F ?1 )\\nwhich ensures that points nearby in Rb will be mapped to points nearby in Ra , a property referred\\nto as similarity preservation in (Lawrence & Candela, 2006). The back constrained GP-LVM simultaneously ensures that the downward mapping (analogous to F ) is regular, thereby additionally\\nimplementing what its authors refer to as dissimilarity preservation. Finally, the similarity between\\nsmoothness (required of F and F ?1 in Definition 2.1) and regularity (imposed on the downward and\\nupward mappings by the GP prior in the back constrained GP-LVM) complete the analogy. There is\\nalso an alternative, more direct motivation for diffeomorphic mappings in the context of dimensionality reduction, however. In particular, a diffeomorphic mapping has the property that it does not\\nlose any information. That is, given the mapping itself and the lower dimensional representation of\\nthe data set, it is always possible to reconstruct the original data.\\nThere has been significant interest from within the image processing community, in the construction\\nof diffeomorphic mappings for the purpose of image warping (Dupuis & Grenander, 1998; Joshi\\n& Miller, 2000; Karac?ali & Davatzikos, 2003). The reason for this can be understood as follows.\\nLet I : U ? R3 represent the RGB values of an image, where U ? R2 is the image plane. If we\\nnow define the warped version of I to be I ? W , then we can guarantee that the warp is topology\\npreserving, i.e. that it does not ?tear? the image, by ensuring the W be a diffeomorphism U ? U .\\nThe following two main approaches to constructing such diffeomorphisms have been taken by the\\nimage processing community, the first of which we mention for reference, while the second forms\\nthe basis of Diffeomap. It is a notable aside that there seem to be no image warping algorithms\\nanalogous to the back constrained GP-LVM, in which regular forward and inverse mappings are\\nsimultaneously constructed.\\n1. Enforcement of the constraint that |J(W )|, the determinant of the Jacobian of the mapping, be positive everywhere. This approach has been successfully applied to the problem\\nof warping 3D magnetic resonance images (Karac?ali & Davatzikos, 2003), for example,\\nbut a key ingredient of that success was the fact that the authors defined the mapping W\\nnumerically on a regular grid. For the high dimensional cases relevant to dimensionality\\nreduction however, such a numerical grid is highly computationally unattractive.\\n2. Recasting the problem of constructing W as an Eulerian flow problem (Dupuis & Grenander, 1998; Joshi & Miller, 2000). This approach is the focus of the next section.\\n2\\n\\n\\fR\\n\\nR\\n?(x, 1) = ?(x)\\n(s, ?(x, s))\\n(1, v(?(x, s), s))\\n\\nx\\nt\\n0\\n\\ns\\n\\n1\\n\\nFigure 1: The relationship between v(?, ?), ?(?, ?) and ?(?) for the one dimensional case ? : R ? R.\\n\\n2.1\\n\\nDiffeomorphisms via Flow Fields\\n\\nThe idea here is to indirectly define the mapping of interest, call it ? : Ra ? Ra , by way of a ?time?\\nindexed velocity field v : Ra ? R ? Ra . In particular we write ?(x) = ?(x, 1), where\\nZ t\\nv(?(x, s), s)ds.\\n(2)\\n?(x, t) = x +\\ns=0\\n\\nThis choice of ? satisfies the following Eulerian transport equation with boundary conditions:\\n??(x, s)\\n= v(?(x, s), s),\\n?s\\n\\n?(x, 0) = x.\\n\\n(3)\\n\\nThe role of v is to transport a given point x from its original location at time 0 to its mapped location\\n?(x, 1) by way of a trajectory whose position and tangent vector at time s are given by ?(x, s) and\\nv(?(x, s), s), respectively (see Figure 1). The point of this construction is that if v satisfies certain\\nregularity properties, then the mapping ? will be a diffeomorphism. This fact has been proven in a\\nnumber of places?one particularly accessible example is (Dupuis & Grenander, 1998), where the\\nnecessary conditions are provided for the three dimensional case along with a proof that the induced\\nmapping is a diffeomorphism. Generalizing the result to higher dimensions is straightforward?this\\nfact is stated in (Dupuis & Grenander, 1998) along with the basic idea of how to do so.\\nWe now offer an intuitive argument for the result. Consider Figure 1, and imagine adding a new\\nstarting point x? , along with its associated trajectory. It is clear that for the mapping ? to be a\\ndiffeomorphism, then for any such pair of points x and x? , the associated trajectories must not\\ncollide. This is because the two trajectories would be identical after the collision, x and x? would\\nmap to the same point, and hence the mapping would not be invertible. But if v is sufficiently regular\\nthen such collisions cannot occur.\\n\\n3\\n\\nDiffeomorphic Dimensionality Reduction\\n\\nThe framework of Eulerian flow fields which we have just introduced provides an elegant means\\nof constructing diffeomorphic mappings Ra ? Ra , but for dimensionality reduction we require\\nadditional ingredients, which we now introduce. The basic idea is to construct a diffeomorphic\\nmapping in such a way that it maps our data set near to a subspace of Ra , and then to project onto\\nthis subspace. The subspace we use, call it Sb , is the b-dimensional one spanned by the first b\\ncanonical basis vectors of Ra . Let P(a?b) : Ra ? Rb be the projection operator which extracts the\\nfirst b components of the vector it is applied to, i.e.\\nP(a?b) x = (I Z) x,\\n\\n(4)\\n\\nwhere I ? Ra?a is the identity matrix and Z ? Ra?b?a is a matrix of zeros. We can now write the\\nmapping ? : Ra ? Rb which we propose for dimensionality reduction as\\n?(x) = P(a?b) ?(x, 1),\\n3\\n\\n(5)\\n\\n\\fwhere ? is given by (2). We choose each component of v at each time to belong to a reproducing\\nkernel Hilbert Space (RKHS) H, so that v(?, t) ? Ha , t ? [0, 1]. If we define the norm1\\na \\n\\n2\\nX\\n\\n\\n2\\nkv(?, t)kHa ,\\n(6)\\n\\n[v(?, t)]j \\n ,\\nH\\n\\nj=1\\n\\n2\\n\\nthen kv(?, t)kHa < ?, ?t ? [0, 1] is a sufficient condition which guarantees that ? is a diffeomorphism, provided that some technical conditions are satisfied (Dupuis & Grenander, 1998; Joshi\\n& Miller, 2000). In particular v need not be regular in its second argument. For dimensionality\\nreduction we propose to construct v as the minimizer of\\nZ 1\\nm\\nX\\n2\\nkv(?, t)kHd dt +\\nL (?(xj )) ,\\n(7)\\nO=?\\nt=0\\n\\nj=1\\n\\n+\\n\\nwhere ? ? R is a regularization parameter. Here, L measures the squared distance to our b\\ndimensional linear subspace of interest Sb , i.e.\\nL(x) =\\n\\na\\nX\\n\\n2\\n\\n[x]d .\\n\\n(8)\\n\\nd=b+1\\n\\nNote that this places special importance on the first b dimensions of the input space of interest?\\naccordingly we make the natural and important preprocessing step of applying PCA such that as\\nmuch as possible of the variance of the data is captured in these first b dimensions.\\n3.1\\n\\nImplementation\\n\\nOne can show that the minimizer in v of (7) takes the form\\n[v(?, t)]d =\\n\\nm\\nX\\n\\n[?d (t)]j k(?(xj , t), ?),\\n\\nd = 1 . . . a,\\n\\n(9)\\n\\nj=1\\n\\nwhere k is the reproducing kernel of H and ?d is a function [0, 1] ? Rm . This was proven directly\\nfor a similar specific case (Joshi & Miller, 2000), but we note in passing that it follows immediately\\nfrom the celebrated representer theorem of RKHS?s (Sch?olkopf et al., 2001), by considering a fixed\\ntime t. Hence, we have simplified the problem of determining v to one of determining m trajectories\\n?(xj , ?). This is because not only does (9) hold, but we can use standard manipulations (in the\\ncontext of kernel ridge regression, for example) to determine that for a given set of such trajectories,\\n?d (t) = K(t)?1 ud (t),\\nm?m\\n\\nd = 1, 2, . . . , a,\\n\\n(10)\\n\\nm\\n\\nwhere t ? [0, 1], K(t) ? R\\n, ud (t) ? R and we have let [K(t)]j,k = k(?(xj , t), ?(xk , t))\\nalong with [ud (t)]j = ?t ?(xj , t). Note that the invertibility of K(t) is guaranteed for certain kernel\\nfunctions (including the Gaussian kernel which we employ in all our Experiments, see Section 4),\\nprovided that the set ?(xj , t) are distinct. Hence, one can verify using (9), (10) and the reproducing\\nproperty of k in H (i.e. the fact that hf, k(x, ?)iH = f (x), ?f ? H), that for the optimal v,\\n2\\n\\nkv(?, t)kHa =\\n\\na\\nX\\n\\nud (t)? K(t)?1 ud (t).\\n\\n(11)\\n\\nd=1\\n\\nThis allows us to write our objective (7) in terms of the m trajectories mentioned above:\\nZ 1 X\\na\\nm X\\na\\nX\\n2\\nO=?\\nud (t)? K(t)?1 ud (t) +\\n[?(xj , 1)]d .\\nt=0 d=1\\n\\n(12)\\n\\nj=1 d=b+1\\n\\nSo far no approximations have been made, and we have constructed an optimal finite dimensional\\nbasis for v(?, t). The second argument of v is not so easily dealt with however, so as an approximate\\nby discretizing the interval [0, 1]. In particular, we let tk = k?, k = 0, 1, . . . , p, where ? = 1/p,\\nand make the approximation ?t=tk ?(xj , t) = (?(xj , tk ) ? ?(xj , tk?1 )) /?. By making the further\\n1\\n\\nSquare brackets w/ subscripts denote matrix elements, and colons denote entire rows or columns.\\n\\n4\\n\\n\\f0.9\\n\\n0.8\\n\\n0.7\\n\\n0.6\\n\\n0.5\\n\\n(d)\\n\\n0.4\\n\\n(c)\\n(b)\\n\\n0.3\\n\\n0.2\\n\\n0.1\\n\\n0\\n0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\n1\\n\\n(a)\\n\\n(b)\\n\\n(c)\\n\\n(d)\\n\\nFigure 2: Dimensionality reduction of motion capture data. (a) The data mapped from 102 to\\n2 dimensions using Diffeomap (the line shows the temporal order in which the input data were\\nrecorded). (b)-(d) Three rendered input points corresponding to the marked locations in (a).\\nR tk\\napproximation t=t\\nK(t)?1 dt = ?K(tk?1 )?1 , and substituting into (12) we obtain the first form\\nk?1\\nof our problem which is finite dimensional and hence readily optimized, i.e. the minimization of\\np\\na\\nb\\nX\\n? XX\\n?\\n(?k,d ? ?k?1,d ) K(tk )?1 (?k,d ? ?k?1,d ) +\\nk?p,d k2\\n?\\n\\n(13)\\n\\nd=a+1\\n\\nd=1 k=1\\n\\nwith respect to ?k,d ? Rm for k = 1, 2, . . . , p and d = 1, 2, . . . , a, where [?k,d ]j = [?(xj , tk )]d .\\n3.2\\n\\nA Practical Reduced Set Implementation\\n\\nA practical problem with (13) is the computationally expensive matrix inverse. In practice we reduce\\nthis burden by employing a reduced set expansion which replaces the sum over 1, 2, . . . , m in (9)\\nwith a sum over a randomly selected subset I, thereby using |I| = n basis functions to represent\\nv(?, t). In this case it is possible to show using the reproducing property of k(?, ?) that the resulting\\nobjective function is identical to (13), but with the matrix K(tk )?1 replaced by the expression\\nKm,n (Kn,m Km,n )\\n?\\nKn,m\\n\\n?1\\n\\n?1\\n\\nKn,n (Kn,m Km,n )\\n\\nKn,m ,\\n\\n(14)\\n\\nm?n\\n\\nwhere Km,n =\\n? R\\nis the sub-matrix of K(tk ) formed by taking all of the rows, but\\nonly those columns given by I. Similarly, Kn,n ? Rn?n is the square sub-matrix of K(tk ) formed\\nby taking a subset of both the rows and columns, namely those given by I. For optimization we\\nalso use the gradients of the above expression, the derivation of which we have omitted for brevity.\\nNote however that by factorizing appropriately, the computation of the objective function and its\\ngradients can be performed with an asymptotic time complexity of n2 (m + a).\\n\\n4\\n\\nExperiments\\n\\nIt is difficult to objectively compare dimensionality reduction algorithms, as there is no universally\\nagreed upon measure of performance. Algorithms which are generalizations or variations of older\\nones may be compared side by side with their predecessors, but this is not the case with our new\\nalgorithm, Diffeomap. Hence, in this section we attempt to convince the reader of the utility of our\\napproach by visually presenting our results on as many and as varied realistic problems as space\\npermits, while providing pointers to comparable results from other authors. For all experiments\\nwe fixed the parameters which trade off between computational speed and accuracy, i.e. we set the\\ntemporal resolution p = 20, and the number of\\n\\u0001 basis functions n = 300. We used a Gaussian kernel\\nfunction k(x, y) = exp ?kx ? yk2 /(2? 2 ) , and tuned the ? parameter manually along with the\\nregularization parameter ?. For optimization we used a conjugate gradient type method2 fixed to\\n1000 iterations and with starting point [?k,d ]j = [xj ]d , k = 1, 2, . . . p.\\n2\\n\\nCarl Rasmussen?s minimize.m, which is freely available from http://www.kyb.mpg.de/?carl.\\n\\n5\\n\\n\\fa\\n?\\n\\\"\\ne\\ni\\n1\\no\\n@\\nu\\n(a)\\n\\n(b)\\n\\n(c)\\n\\nFigure 3: Vowel data mapped from 24 to 2 dimensions using (a) PCA and (b)-(c) Diffeomap. Plots\\n(b) and (c) differ only in the parameter settings of Diffeomap, with (b) corresponding to minimal\\none nearest neighbor errors in the low dimensional space?see Section 4.2 for details.\\n4.1\\n\\nMotion Capture Data\\n\\nThe first data set we consider consists of the coordinates in R3 of a set of markers placed on a person\\nbreaking into a run, sampled at a constant frequency, resulting in m = 217 data points in a = 102\\ndimensions, which we mapped to b = 2 dimensions using Diffeomap (see Figure 2). This data set\\nis freely available from http://accad.osu.edu/research/mocap/mocap_data.htm\\nas Figure 1 Run, and was also considered in (Lawrence & Candela, 2006), where it was shown\\nthat while the original GP-LVM fails to correctly discover the periodic component of the sequence,\\nthe back constrained version maps poses in the same part of the subject?s step cycle nearby to\\neach other, while simultaneously capturing variations in the inclination of the subject. Diffeomap\\nalso succeeded in this sense, and produced results which are competitive with those of the back\\nconstrained GP-LVM.\\n4.2\\n\\nVowel Data\\n\\nIn this next example we consider a data set of a = 24 features (cepstral coefficients and delta\\ncepstral coefficients) of a single speaker performing nine different vowels 300 times per vowel,\\nacquired as training data for a vocal joystick system (Bilmes & et.al., 2006), and publicly available\\nin pre-processed form from http://www.dcs.shef.ac.uk/?neil/fgplvm/. Once again\\nwe used Diffeomap to map the data to b = 2 dimensions, as depicted in Figure 3. We also depict\\nthe poor result of linear PCA, in order to rule out the hypothesis that it is merely the PCA based\\ninitialization of Diffeomap (mentioned after equation (8) on page 4) which does most of the work.\\nThe results in Figure 3 are directly comparable to those provided in (Lawrence & Candela, 2006)\\nfor the GP-LVM, back constrained GP-LVM, and Isomap (Tenenbaum et al., 2000). Visually, the\\nDiffeomap result appears to be superior to those of the GP-LVM and Isomap, and comparable to the\\nback constrained GP-LVM. We also measured the performance of a one nearest neighbor classifier\\napplied to the mapped data in R2 . For the best choice of the parameters ? and ?, Diffeomap made\\n140 errors, which is favorable to the figures quoted for Isomap (458), the GP-LVM (226) and the\\nback constrained GP-LVM (155) in (Lawrence & Candela, 2006). We emphasize however that this\\nmeasure of performance is at best a rough one, since by manually varying our choice of the parameters ? and ?, we were able to obtain a result (Figure 3 (c)) which, although leads to a significantly\\nhigher number of such errors (418), is arguably superior from a qualitative perspective to the result\\nwith minimal errors (Figure 3 (b)).\\n4.3\\n\\nUSPS Handwritten Digits\\n\\nWe now consider the USPS database of handwritten digits (Hull, 1994). Following the methodology of the stochastic neighbor embedding (SNE) and GP-LVM papers (Hinton & Roweis, 2003;\\nLawrence, 2004), we take 600 images per class from the five classes corresponding to digits 0, 1, 2,\\n3, 4. Since the images are in gray scale and a resolution of 16 by 16 pixels, this results in a data set\\nof m = 3000 examples in a = 256 dimensions, which we again mapped to b = 2 dimensions as\\ndepicted in Figure 4. The figure shows the individual points color coded according to class, along\\n6\\n\\n\\f(a)\\n\\n(b)\\n\\nFigure 4: USPS handwritten digits 0-4 mapped to 2 dimensions using Diffeomap. (a) Mapped points\\ncolor coded by class label. (b) A composite image of the mapped data?see Section 4.3 for details.\\n\\nwith a composite image formed by sequentially drawing each digit in random order at its mapped\\nlocation, but only if it would not obscure a previously drawn digit. Diffeomap manages to arrange\\nthe data in a manner which reveals such image properties as digit angle and stroke thickness. At the\\nsame time the classes are reasonably well separated, with the exception of the ones which are split\\ninto two clusters depending on the angle. Although unfortunate, we believe that this splitting can\\nbe explained by the fact that (a) the left- and right-pointing ones are rather dissimilar in input space,\\nand (b) the number of fairly vertical ones which could help to connect the left- and right-pointing\\nones is rather small. Diffeomap seems to produce a result which is superior to that of the GP-LVM\\n(Lawrence, 2004), for example, but may be inferior to that of the SNE (Hinton & Roweis, 2003). We\\nbelieve this is due to the fact that the nearest neighbor graph used by SNE is highly appropriate to the\\nUSPS data set. This is indicated by the fact that a nearest neighbor classifier in the 256 dimensional\\ninput space is known to perform strongly, with numerous authors having reported error rates of less\\nthan 5% on the ten class classification problem.\\n4.4\\n\\nNIPS Text Data\\n\\nFinally, we present results on the text data of papers from the NIPS conference proceedings volumes\\n0-12, which can be obtained from http://www.cs.toronto.edu/?roweis/data.html.\\nThis experiment is intended to address the natural concern that by working in the input space rather\\nthan on a nearest neighbor graph, for example, Diffeomap may have difficulty with very high dimensional data. Following (Hinton & Roweis, 2003; Song et al., 2008) we represent the data as a word\\nfrequency vs. document matrix in which the author names are treated as words but weighted up by\\na factor 20 (i.e. an author name is worth 20 words). The result is a data set of m = 1740 papers\\nrepresented in a = 13649 words + 2037 authors = 15686 dimensions. Note however that the input\\ndimensionality is effectively reduced by the PCA preprocessing step to m ? 1 = 1739, that being\\nthe rank of the centered covariance matrix of the data.\\nAs this data set is difficult to visualize without taking up large amounts of space, we have included\\nthe results in the supplementary material which accompanies our NIPS submission. In particular,\\nwe provide a first figure which shows the data mapped to b = 2 dimensions, with certain authors (or\\ngroups of authors) color coded?the choice of authors and their corresponding color codes follows\\nprecisely those of (Song et al., 2008). A second figure shows a plain marker drawn at the mapped\\nlocations corresponding to each of the papers. This second figure also contains the paper title and\\nauthors of the corrsponding papers however, which are revealed when the user moves the mouse\\nover the marked locations. Hence, this second figure allows one to browse the NIPS collection con7\\n\\n\\ftextually. Since the mapping may be hard to judge, we note in passing that the correct classification\\nrate of a one nearest neighbor classifier applied to the result of Diffeomap was 48%, which compares\\nfavorably to the rate of 33% achieved by linear PCA (which we use for preprocessing). To compute\\nthis score we treated authors as classes, and considered only those authors who were color coded\\nboth in our supplementary figure and in (Song et al., 2008).\\n\\n5\\n\\nConclusion\\n\\nWe have presented an approach to dimensionality reduction which is based on the idea that the mapping between the lower and higher dimensional spaces should be diffeomorphic. We provided a\\njustification for this approach, by showing that the common intuition that dimensionality reduction\\nalgorithms should approximately preserve pairwise distances of a given data set is closely related to\\nthe idea that the mapping induced by the algorithm should be a diffeomorphism. This realization\\nallowed us to take advantage of established mathematical machinery in order to convert the dimensionality reduction problem into a so called Eulerian flow problem, the solution of which is guaranteed to generate a diffeomorphism. Requiring that the mapping and its inverse both be smooth is\\nreminiscent of the GP-LVM algorithm (Lawrence & Candela, 2006), but has the advantage in terms\\nof statistical strength that we need not separately estimate a mapping in each direction. We showed\\nresults of our algorithm, Diffeomap, on a relatively small motion capture data set, a larger vowel\\ndata set, the USPS image data set, and finally the rather high dimensional data set derived from the\\ntext corpus of NIPS papers, with successes in all cases. Since our new approach performs well in\\npractice while being significantly different to all previous approaches to dimensionality reduction, it\\nhas the potential to lead to a significant new direction in the field.\\n\\nReferences\\nBilmes, J., & et.al. (2006). The Vocal Joystick. Proc. IEEE Intl. Conf. on Acoustic, Speech and Signal Processing. Toulouse, France.\\nCox, T., & Cox, M. (1994). Multidimensional scaling. London, UK: Chapman & Hall.\\nDeMers, D., & Cottrell, G. (1993). Non-linear dimensionality reduction. NIPS 5 (pp. 580?587). Morgan\\nKaufmann, San Mateo, CA.\\nDuda, R. O., Hart, P. E., & Stork, D. G. (2000). Pattern classification. New York: Wiley. 2nd Edition.\\nDupuis, P., & Grenander, U. (1998). Variational problems on flows of diffeomorphisms for image matching.\\nQuarterly of Applied Mathematics, LVI, 587?600.\\nHinton, G., & Roweis, S. (2003). Stochastic neighbor embedding. In S. T. S. Becker and K. Obermayer (Eds.),\\nAdvances in neural information processing systems 15, 833?840. Cambridge, MA: MIT Press.\\nHull, J. J. (1994). A database for handwritten text recognition research. IEEE Trans. Pattern Anal. Mach.\\nIntell., 16, 550?554.\\nJoshi, S. C., & Miller, M. I. (2000). Landmark matching via large deformation diffeomorphisms. IEEE Transactions on Image Processing, 9, 1357?1370.\\nKarac?ali, B., & Davatzikos, C. (2003). Topology preservation and regularity in estimated deformation fields.\\nInformation Processing in Medical Imaging (pp. 426?437).\\nLawrence, N. D. (2004). Gaussian process latent variable models for visualisation of high dimensional data. In\\nS. Thrun, L. Saul and B. Sch?olkopf (Eds.), Nips 16. Cambridge, MA: MIT Press.\\nLawrence, N. D., & Candela, J. Q. (2006). Local distance preservation in the GP-LVM through back constraints.\\nIn International conference on machine learning, 513?520. ACM.\\nSch?olkopf, B., Herbrich, R., & Smola, A. J. (2001). A generalized representer theorem. Proc. of the 14th\\nAnnual Conf. on Computational Learning Theory (pp. 416?426). London, UK: Springer-Verlag.\\nSong, L., Smola, A., Borgwardt, K., & Gretton, A. (2008). Colored maximum variance unfolding. In J. Platt,\\nD. Koller, Y. Singer and S. Roweis (Eds.), Nips 20, 1385?1392. Cambridge, MA: MIT Press.\\nTenenbaum, J. B., de Silva, V., & Langford, J. C. (2000). A global geometric framework for nonlinear dimensionality reduction. Science, 290, 2319?2323.\\nvan der Maaten, L. J. P., Postma, E., & van den Herik, H. (2008). Dimensionality reduction: A comparative\\nreview. In T. Ertl (Ed.), Submitted to neurocognition. Elsevier.\\nVenna, J. (2007). Dimensionality reduction for visual exploration of similarity structures. Doctoral dissertation,\\nHelsinki University of Technology.\\n\\n8\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ed0a496",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ed0a496",
        "outputId": "d4028b42-db8e-4e0b-93b9-132e2b4df3fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc4edf5a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "dc4edf5a",
        "outputId": "50d620df-5e11-477a-d587-75f49693a73c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id               0\n",
              "year             0\n",
              "title            0\n",
              "event_type    4335\n",
              "pdf_name         0\n",
              "abstract         0\n",
              "paper_text       0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>year</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>title</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>event_type</th>\n",
              "      <td>4335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pdf_name</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abstract</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>paper_text</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5db6180",
      "metadata": {
        "id": "c5db6180"
      },
      "source": [
        "# Preprocessing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "354e60ac",
      "metadata": {
        "id": "354e60ac"
      },
      "source": [
        "# Working With \"paper text\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d755783",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "4d755783",
        "outputId": "d0918a3c-bea5-45b4-8056-e07658a5969e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABASE\\nAND ITS APPLICATIONS\\nHisashi Suzuki and Suguru Arimoto\\nOsaka University, Toyonaka, Osaka 560, Japan\\nABSTRACT\\nAn efficient method of self-organizing associative databases is proposed together with\\napplications to robot eyesight systems. The proposed databases can associate any input\\nwith some output. In the first half part of discussion, an algorithm of self-organization is\\nproposed. From an aspect of hardware, it produces a new style of neural network. In the\\nlatter half part, an applicability to handwritten letter recognition and that to an autonomous\\nmobile robot system are demonstrated.\\n\\nINTRODUCTION\\nLet a mapping f : X -+ Y be given. Here, X is a finite or infinite set, and Y is another\\nfinite or infinite set. A learning machine observes any set of pairs (x, y) sampled randomly\\nfrom X x Y. (X x Y means the Cartesian product of X and Y.) And, it computes some\\nestimate j : X -+ Y of f to make small, the estimation error in some measure.\\nUsually we say that: the faster the decrease of estimation error with increase of the number of samples, the better the learning machine. However, such expression on performance\\nis incomplete. Since, it lacks consideration on the candidates of J of j assumed preliminarily. Then, how should we find out good learning machines? To clarify this conception,\\nlet us discuss for a while on some types of learning machines. And, let us advance the\\nunderstanding of the self-organization of associative database .\\n. Parameter Type\\nAn ordinary type of learning machine assumes an equation relating x\\'s and y\\'s with\\nparameters being indefinite, namely, a structure of f. It is equivalent to define implicitly a\\nset F of candidates of\\n(F is some subset of mappings from X to Y.) And, it computes\\nvalues of the parameters based on the observed samples. We call such type a parameter\\ntype.\\nFor a learning machine defined well, if F 3 f, j approaches f as the number of samples\\nincreases. In the alternative case, however, some estimation error remains eternally. Thus,\\na problem of designing a learning machine returns to find out a proper structure of f in this\\nsense.\\nOn the other hand, the assumed structure of f is demanded to be as compact as possible\\nto achieve a fast learning. In other words, the number of parameters should be small. Since,\\nif the parameters are few, some j can be uniquely determined even though the observed\\nsamples are few. However, this demand of being proper contradicts to that of being compact.\\nConsequently, in the parameter type, the better the compactness of the assumed structure\\nthat is proper, the better the learning machine. This is the most elementary conception\\nwhen we design learning machines .\\n\\n1.\\n\\n. Universality and Ordinary Neural Networks\\nNow suppose that a sufficient knowledge on f is given though J itself is unknown. In\\nthis case, it is comparatively easy to find out proper and compact structures of J. In the\\nalternative case, however, it is sometimes difficult. A possible solution is to give up the\\ncompactness and assume an almighty structure that can cover various 1\\'s. A combination\\nof some orthogonal bases of the infinite dimension is such a structure. Neural networks 1 ,2\\nare its approximations obtained by truncating finitely the dimension for implementation.\\n\\n? American Institute of Physics 1988\\n\\n\\x0c768\\nA main topic in designing neural networks is to establish such desirable structures of 1.\\nThis work includes developing practical procedures that compute values of coefficients from\\nthe observed samples. Such discussions are :flourishing since 1980 while many efficient methods have been proposed. Recently, even hardware units computing coefficients in parallel\\nfor speed-up are sold, e.g., ANZA, Mark III, Odyssey and E-1.\\nNevertheless, in neural networks, there always exists a danger of some error remaining\\neternally in estimating /. Precisely speaking, suppose that a combination of the bases of a\\nfinite number can define a structure of 1 essentially. In other words, suppose that F 3 /, or\\n1 is located near F. In such case, the estimation error is none or negligible. However, if 1\\nis distant from F, the estimation error never becomes negligible. Indeed, many researches\\nreport that the following situation appears when 1 is too complex. Once the estimation\\nerror converges to some value (> 0) as the number of samples increases, it decreases hardly\\neven though the dimension is heighten. This property sometimes is a considerable defect of\\nneural networks .\\n. Recursi ve Type\\nThe recursive type is founded on another methodology of learning that should be as\\nfollows. At the initial stage of no sample, the set Fa (instead of notation F) of candidates\\nof I equals to the set of all mappings from X to Y. After observing the first sample\\n(Xl, Yl) E X x Y, Fa is reduced to Fi so that I(xt) = Yl for any I E F. After observing\\nthe second sample (X2\\' Y2) E X x Y, Fl is further reduced to F2 so that i(xt) = Yl and\\nI(X2) = Y2 for any I E F. Thus, the candidate set F becomes gradually small as observation\\nof samples proceeds. The after observing i-samples, which we write\\nis one of the most\\nlikelihood estimation of 1 selected in fi;. Hence, contrarily to the parameter type, the\\nrecursive type guarantees surely that j approaches to 1 as the number of samples increases.\\nThe recursive type, if observes a sample (x\" yd, rewrites values 1,-l(X),S to I,(x)\\'s for\\nsome x\\'s correlated to the sample. Hence, this type has an architecture composed of a rule\\nfor rewriting and a free memory space. Such architecture forms naturally a kind of database\\nthat builds up management systems of data in a self-organizing way. However, this database\\ndiffers from ordinary ones in the following sense. It does not only record the samples already\\nobserved, but computes some estimation of l(x) for any x E X. We call such database an\\nassociative database.\\nThe first subject in constructing associative databases is how we establish the rule for\\nrewri ting. For this purpose, we adap t a measure called the dissimilari ty. Here, a dissimilari ty\\nmeans a mapping d : X x X -+ {reals > O} such that for any (x, x) E X x X, d(x, x) > 0\\nwhenever l(x) t /(x). However, it is not necessarily defined with a single formula. It is\\ndefinable with, for example, a collection of rules written in forms of \"if? .. then?? .. \"\\nThe dissimilarity d defines a structure of 1 locally in X x Y. Hence, even though\\nthe knowledge on f is imperfect, we can re:flect it on d in some heuristic way. Hence,\\ncontrarily to neural networks, it is possible to accelerate the speed of learning by establishing\\nd well. Especially, we can easily find out simple d\\'s for those l\\'s which process analogically\\ninformation like a human. (See the applications in this paper.) And, for such /\\'s, the\\nrecursive type shows strongly its effectiveness.\\nWe denote a sequence of observed samples by (Xl, Yd, (X2\\' Y2),???. One of the simplest\\nconstructions of associative databases after observing i-samples (i = 1,2,.,,) is as follows.\\n\\ni\\n\\ni\"\\n\\nI,\\n\\nAlgorithm 1. At the initial stage, let So be the empty set. For every i =\\n1,2\" .. , let i,-l(x) for any x E X equal some y* such that (x*,y*) E S,-l and\\n\\nd(x, x*) =\\n\\nmin\\n(%,y)ES.-t\\n\\nd(x, x) .\\n\\nFurthermore, add (x\" y,) to S;-l to produce Sa, i.e., S, = S,_l U {(x\"\\n\\n(1)\\n\\ny,n.\\n\\n\\x0c769\\n\\nAnother version improved to economize the memory is as follows.\\n\\nAlgorithm 2, At the initial stage, let So be composed of an arbitrary element\\nin X x Y. For every i = 1,2\"\", let ii-lex) for any x E X equal some y. such\\nthat (x?, y.) E Si-l and\\nd(x, x?) =\\n\\nmin\\n\\nd(x, x) .\\n\\n(i,i)ES.-l\\n\\nFurthermore, if ii-l(Xi) # Yi then let Si = Si-l, or add (Xi, Yi) to Si-l to\\nproduce Si, i.e., Si = Si-l U {(Xi, Yi)}\\'\\nIn either construction, ii approaches to f as i increases. However, the computation time\\ngrows proportionally to the size of Si. The second subject in constructing associative\\ndatabases is what addressing rule we should employ to economize the computation time. In\\nthe subsequent chapters, a construction of associative database for this purpose is proposed.\\nIt manages data in a form of binary tree.\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABASE\\nGiven a sample sequence (Xl, Yl), (X2\\' Y2), .. \" the algorithm for constructing associative\\ndatabase is as follows.\\n\\nAlgorithm 3,\\'\\n\\nStep I(Initialization): Let (x[root], y[root]) = (Xl, Yd. Here, x[.] and y[.] are\\nvariables assigned for respective nodes to memorize data.. Furthermore, let t = 1.\\nStep 2: Increase t by 1, and put x, in. After reset a pointer n to the root, repeat\\nthe following until n arrives at some terminal node, i.e., leaf.\\nNotations nand\\nd(xt, x[n)), let n\\n\\nn mean the descendant nodes of n.\\n=n. Otherwise, let n =n.\\n\\nIf d(x\" r[n)) ~\\n\\nStep 3: Display yIn] as the related information. Next, put y, in. If yIn] = y\" back\\nto step 2. Otherwise, first establish new descendant nodes n and n. Secondly,\\nlet\\n\\n(x[n], yIn))\\n(x[n], yIn))\\n\\n(x[n], yIn)),\\n(Xt, y,).\\n\\n(2)\\n(3)\\n\\nFinally, back to step 2. Here, the loop of step 2-3 can be stopped at any time\\nand also can be continued.\\nNow, suppose that gate elements, namely, artificial \"synapses\" that play the role of branching by d are prepared. Then, we obtain a new style of neural network with gate elements\\nbeing randomly connected by this algorithm.\\n\\nLETTER RECOGNITION\\nRecen tly, the vertical slitting method for recognizing typographic English letters3 , the\\nelastic matching method for recognizing hand written discrete English letters4 , the global\\ntraining and fuzzy logic search method for recognizing Chinese characters written in square\\nstyleS, etc. are published. The self-organization of associative database realizes the recognition of handwritten continuous English letters.\\n\\n\\x0c770\\n\\n9 /wn\"\\n\\nNOV\\n\\n~ ~ ~ -xk :La.t\\n\\n~~ ~ ~~~\\n\\ndw1lo\\'\\n\\n~~~~~of~~\\n\\n~~~ 4,-?~~4Fig. 1. Source document.\\n2~~---------------\\'\\n\\nlOO~---------------\\'\\n\\nH\\n\\no\\n\\no\\nFig. 2. Windowing.\\n\\n1000\\n\\n2000\\n\\n3000\\n\\n4000\\n\\nNumber of samples\\n\\no\\n\\n1000\\n\\n2000\\n\\n3000\\n\\n4000\\n\\nNUAlber of sampl es\\n\\nFig. 3. An experiment result.\\n\\nAn image scanner takes a document image (Fig. 1). The letter recognizer uses a parallelogram window that at least can cover the maximal letter (Fig. 2), and processes the\\nsequence of letters while shifting the window. That is, the recognizer scans a word in a\\nslant direction. And, it places the window so that its left vicinity may be on the first black\\npoint detected. Then, the window catches a letter and some part of the succeeding letter.\\nIf recognition of the head letter is performed, its end position, namely, the boundary line\\nbetween two letters becomes known. Hence, by starting the scanning from this boundary\\nand repeating the above operations, the recognizer accomplishes recursively the task. Thus\\nthe major problem comes to identifying the head letter in the window.\\nConsidering it, we define the following.\\n? Regard window images as x\\'s, and define X accordingly.\\n? For a (x, x) E X x X, denote by B a black point in the left area from the boundary on\\nwindow image X. Project each B onto window image x. Then, measure the Euclidean\\ndistance 6 between fj and a black point B on x being the closest to B. Let d(x, x) be\\nthe summation of 6\\'s for all black points B\\'s on x divided by the number of B\\'s.\\n? Regard couples of the \"reading\" and the position of boundary as y\\'s, and define Y\\naccordingly.\\nAn operator teaches the recognizer in interaction the relation between window image and\\nreading& boundary with algorithm 3. Precisely, if the recalled reading is incorrect, the\\noperator teaches a correct reading via the console. Moreover, if the boundary position is\\nincorrect, he teaches a correct position via the mouse.\\nFig. 1 shows partially a document image used in this experiment. Fig. 3 shows the\\nchange of the number of nodes and that of the recognition rate defined as the relative\\nfrequency of correct answers in the past 1000 trials. Speciiications of the window are height\\n= 20dot, width = 10dot, and slant angular = 68deg. In this example, the levels of tree\\nwere distributed in 6-19 at time 4000 and the recognition rate converged to about 74%.\\nExperimentally, the recognition rate converges to about 60-85% in most cases, and to 95% at\\na rare case. However, it does not attain 100% since, e.g., \"c\" and \"e\" are not distinguishable\\nbecause of excessive lluctuation in writing. If the consistency of the x, y-relation is not\\nassured like this, the number of nodes increases endlessly (d. Fig. 3). Hence, it is clever to\\nstop the learning when the recognition rate attains some upper limit. To improve further\\nthe recognition rate, we must consider the spelling of words. It is one of future subjects.\\n\\n\\x0c771\\n\\nOBSTACLE AVOIDING MOVEMENT\\nVarious systems of camera type autonomous mobile robot are reported flourishingly6-1O.\\nThe system made up by the authors (Fig. 4) also belongs to this category. Now, in mathematical methodologies, we solve usually the problem of obstacle avoiding movement as\\na cost minimization problem under some cost criterion established artificially. Contrarily,\\nthe self-organization of associative database reproduces faithfully the cost criterion of an\\noperator. Therefore, motion of the robot after learning becomes very natural.\\nNow, the length, width and height of the robot are all about O.7m, and the weight is\\nabout 30kg. The visual angle of camera is about 55deg. The robot has the following three\\nfactors of motion. It turns less than ?30deg, advances less than 1m, and controls speed less\\nthan 3km/h. The experiment was done on the passageway of wid th 2.5m inside a building\\nwhich the authors\\' laboratories exist in (Fig. 5). Because of an experimental intention, we\\narrange boxes, smoking stands, gas cylinders, stools, handcarts, etc. on the passage way at\\nrandom. We let the robot take an image through the camera, recall a similar image, and\\ntrace the route preliminarily recorded on it. For this purpose, we define the following.\\n? Let the camera face 28deg downward to take an image, and process it through a low\\npass filter. Scanning vertically the filtered image from the bottom to the top, search\\nthe first point C where the luminance changes excessively. Then, su bstitu te all points\\nfrom the bottom to C for white, and all points from C to the top for black (Fig. 6).\\n(If no obstacle exists just in front of the robot, the white area shows the \\'\\'free\\'\\' area\\nwhere the robot can move around.) Regard binary 32 x 32dot images processed thus\\nas x\\'s, and define X accordingly.\\n? For every (x, x) E X x X, let d(x, x) be the number of black points on the exclusive-or\\nimage between x and X.\\n? Regard as y\\'s the images obtained by drawing routes on images x\\'s, and define Y\\naccordingly.\\nThe robot superimposes, on the current camera image x, the route recalled for x, and\\ninquires the operator instructions. The operator judges subjectively whether the suggested\\nroute is appropriate or not. In the negative answer, he draws a desirable route on x with the\\nmouse to teach a new y to the robot. This opera.tion defines implicitly a sample sequence\\nof (x, y) reflecting the cost criterion of the operator.\\n\\n.::l\" !\\n-\\n\\nIibUBe\\n\\n_. -\\n\\n22\\n\\n11\\n\\nRoan\\n\\n12\\n\\n{-\\n\\n13\\n\\nStationary uni t\\n\\nFig. 4. Configuration of\\nautonomous mobile robot system.\\n\\n~\\n\\nI\\n\\n,\\n\\n23\\n\\n24\\n\\nNorth\\n14\\n\\nrmbi Ie unit (robot)\\n\\n-\\n\\nRoan\\n\\ny\\n\\nt\\n\\nFig. 5. Experimental\\nenvironment.\\n\\n\\x0c772\\n\\nWall\\n\\nCamera image\\n\\nPreprocessing\\n\\nA\\n\\n::: !fa\\n\\n?\\n\\nPreprocessing\\n\\n0\\n\\nO\\n\\nCourse\\nsuggest ion\\n\\n??\\n\\n..\\n\\nSearch\\n\\nA\\n\\nFig. 6. Processing for\\nobstacle avoiding movement.\\n\\nx\\n\\nFig. 1. Processing for\\nposition identification.\\nWe define the satisfaction rate by the relative frequency of acceptable suggestions of\\nroute in the past 100 trials. In a typical experiment, the change of satisfaction rate showed\\na similar tendency to Fig. 3, and it attains about 95% around time 800. Here, notice that\\nthe rest 5% does not mean directly the percentage of collision. (In practice, we prevent the\\ncollision by adopting some supplementary measure.) At time 800, the number of nodes was\\n145, and the levels of tree were distributed in 6-17.\\nThe proposed method reflects delicately various characters of operator. For example, a\\nrobot trained by an operator 0 moves slowly with enough space against obstacles while one\\ntrained by another operator 0\\' brushes quickly against obstacles. This fact gives us a hint\\non a method of printing \"characters\" into machines.\\nPOSITION IDENTIFICATION\\nThe robot can identify its position by recalling a similar landscape with the position data\\nto a camera image. For this purpose, in principle, it suffices to regard camera images and\\nposition data as x\\'s and y\\'s, respectively. However, the memory capacity is finite in actual\\ncompu ters. Hence, we cannot but compress the camera images at a slight loss of information.\\nSuch compression is admittable as long as the precision of position identification is in an\\nacceptable area. Thus, the major problem comes to find out some suitable compression\\nmethod.\\nIn the experimental environment (Fig. 5), juts are on the passageway at intervals of\\n3.6m, and each section between adjacent juts has at most one door. The robot identifies\\nroughly from a surrounding landscape which section itself places in. And, it uses temporarily\\na triangular surveying technique if an exact measure is necessary. To realize the former task,\\nwe define the following .\\n? Turn the camera to take a panorama image of 360deg. Scanning horizontally the\\ncenter line, substitute the points where the luminance excessively changes for black\\nand the other points for white (Fig. 1). Regard binary 360dot line images processed\\nthus as x\\'s, and define X accordingly.\\n? For every (x, x) E X x X, project each black point A on x onto x. And, measure the\\nEuclidean distance 6 between A and a black point A on x being the closest to A. Let\\nthe summation of 6 be S. Similarly, calculate S by exchanging the roles of x and X.\\nDenoting the numbers of A\\'s and A\\'s respectively by nand n, define\\n\\n\\x0c773\\n\\nd(x, x) =\\n\\n~(~\\n+ ~).\\n2 n\\nn\\n\\n(4)\\n\\n? Regard positive integers labeled on sections as y\\'s (cf. Fig. 5), and define Y accordingly.\\nIn the learning mode, the robot checks exactly its position with a counter that is reset periodically by the operator. The robot runs arbitrarily on the passageways within 18m area\\nand learns the relation between landscapes and position data. (Position identification beyond 18m area is achieved by crossing plural databases one another.) This task is automatic\\nexcepting the periodic reset of counter, namely, it is a kind of learning without teacher.\\nWe define the identification rate by the relative frequency of correct recalls of position\\ndata in the past 100 trials. In a typical example, it converged to about 83% around time\\n400. At time 400, the number of levels was 202, and the levels oftree were distributed in 522. Since the identification failures of 17% can be rejected by considering the trajectory, no\\npro blem arises in practical use. In order to improve the identification rate, the compression\\nratio of camera images must be loosened. Such possibility depends on improvement of the\\nhardware in the future.\\nFig. 8 shows an example of actual motion of the robot based on the database for obstacle\\navoiding movement and that for position identification. This example corresponds to a case\\nof moving from 14 to 23 in Fig. 5. Here, the time interval per frame is about 40sec.\\n\\n,~. .~ (\\n;~\"i..\\n~\\n\\n\"\\n\\n\"\\n\\n.\\n\\n..I\\n\\nI\\n\\n?\\n?\\n\\n\"\\n\\nI\\'\\n.\\n\\'.1\\nt\\n\\n;\\n\\ni\\n\\n-:\\n, . . , \\'II\\n\\nFig. 8. Actual motion of the robot.\\n\\n\\x0c774\\n\\nCONCLUSION\\nA method of self-organizing associative databases was proposed with the application to\\nrobot eyesight systems. The machine decomposes a global structure unknown into a set of\\nlocal structures known and learns universally any input-output response. This framework\\nof problem implies a wide application area other than the examples shown in this paper.\\nA defect of the algorithm 3 of self-organization is that the tree is balanced well only\\nfor a subclass of structures of f. A subject imposed us is to widen the class. A probable\\nsolution is to abolish the addressing rule depending directly on values of d and, instead, to\\nestablish another rule depending on the distribution function of values of d. It is now under\\ninvestigation.\\n\\nREFERENCES\\n1. Hopfield, J. J. and D. W. Tank, \"Computing with Neural Circuit: A Model/\\'\\n\\nScience 233 (1986), pp. 625-633.\\n2. Rumelhart, D. E. et al., \"Learning Representations by Back-Propagating Errors,\" Nature 323 (1986), pp. 533-536.\\n\\n3. Hull, J. J., \"Hypothesis Generation in a Computational Model for Visual Word\\nRecognition,\" IEEE Expert, Fall (1986), pp. 63-70.\\n4. Kurtzberg, J. M., \"Feature Analysis for Symbol Recognition by Elastic Matching,\" IBM J. Res. Develop. 31-1 (1987), pp. 91-95.\\n\\n5. Wang, Q. R. and C. Y. Suen, \"Large Tree Classifier with Heuristic Search and\\nGlobal Training,\" IEEE Trans. Pattern. Anal. & Mach. Intell. PAMI 9-1\\n(1987) pp. 91-102.\\n6. Brooks, R. A. et al, \"Self Calibration of Motion and Stereo Vision for Mobile\\nRobots,\" 4th Int. Symp. of Robotics Research (1987), pp. 267-276.\\n7. Goto, Y. and A. Stentz, \"The CMU System for Mobile Robot Navigation,\" 1987\\nIEEE Int. Conf. on Robotics & Automation (1987), pp. 99-105.\\n8. Madarasz, R. et al., \"The Design of an Autonomous Vehicle for the Disabled,\"\\nIEEE Jour. of Robotics & Automation RA 2-3 (1986), pp. 117-125.\\n9. Triendl, E. and D. J. Kriegman, \"Stereo Vision and Navigation within Buildings,\" 1987 IEEE Int. Conf. on Robotics & Automation (1987), pp. 1725-1730.\\n10. Turk, M. A. et al., \"Video Road-Following for the Autonomous Land Vehicle,\"\\n1987 IEEE Int. Conf. on Robotics & Automation (1987), pp. 273-279.\\n\\n\\x0c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df['paper_text'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d355d81",
      "metadata": {
        "id": "3d355d81"
      },
      "source": [
        "# Steps to do\n",
        "1 Lower case                                    \n",
        "2 remove HTML tags                                     \n",
        "3 remove special characters and digits                               \n",
        "4 Convert to list from string                                      \n",
        "5 remove stopwords                              \n",
        "6 remove words less than three letters                             \n",
        "7 lemmatize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "265e8feb",
      "metadata": {
        "id": "265e8feb"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49-GBPHcEeGP",
        "outputId": "98afb822-b2f3-4c7d-ece4-3b27541cf0f0"
      },
      "id": "49-GBPHcEeGP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb09de84",
      "metadata": {
        "id": "fb09de84"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "new_stop_words = [\"fig\",\"figure\",\"image\",\"sample\",\"using\",\n",
        "             \"show\", \"result\", \"large\",\n",
        "             \"also\", \"one\", \"two\", \"three\",\n",
        "             \"four\", \"five\", \"seven\",\"eight\",\"nine\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c502c84",
      "metadata": {
        "id": "2c502c84"
      },
      "outputs": [],
      "source": [
        "stop_words = list(stop_words.union(new_stop_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9d810d3",
      "metadata": {
        "id": "d9d810d3"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "def preprocess_text(txt):\n",
        "    # Lower case\n",
        "    txt = txt.lower()\n",
        "    # Remove HTML tags\n",
        "    txt = re.sub(r\"<.*?>\", \" \", txt)\n",
        "    # Remove special characters and digits\n",
        "    txt = re.sub(r\"[^a-zA-Z]\", \" \", txt)\n",
        "    # tokenization\n",
        "    txt = nltk.word_tokenize(txt)\n",
        "    # Remove stopwords\n",
        "    txt = [word for word in txt if word not in stop_words]\n",
        "    # Remove words less than three letters\n",
        "    txt = [word for word in txt if len(word) >= 3]\n",
        "    # Lemmatize\n",
        "    lmtr = WordNetLemmatizer()\n",
        "    txt = [lmtr.lemmatize(word) for word in txt]\n",
        "\n",
        "    return \" \".join(txt)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyoV1PeHEl1u",
        "outputId": "20c3e8f5-9628-485c-df2a-b3d086f69539"
      },
      "id": "gyoV1PeHEl1u",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajPyo-c0ErqW",
        "outputId": "41b7ca32-9e9a-4d97-8892-1a2fd05fdf0c"
      },
      "id": "ajPyo-c0ErqW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab8e6a2b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ab8e6a2b",
        "outputId": "1c64ca87-aca4-4c46-8243-3d5356d439a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'helo word loving moving text html tag'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "preprocess_text(\"HELO word loving moving the to from 99999 *&^ <p>This is a <b>sample</b> text with <i>HTML tags</i>.</p>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af708917",
      "metadata": {
        "id": "af708917"
      },
      "outputs": [],
      "source": [
        "docs = df['paper_text'].apply(lambda x:preprocess_text(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c8940e1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "6c8940e1",
        "outputId": "1480a7e4-e818-44f8-cde1-123f4c293932"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       self organization associative database applica...\n",
              "1       mean field theory layer visual cortex applicat...\n",
              "2       storing covariance associative long term poten...\n",
              "3       bayesian query construction neural network mod...\n",
              "4       neural network ensemble cross validation activ...\n",
              "                              ...                        \n",
              "4995    low rank time frequency synthesis matthieu kow...\n",
              "4996    state space model decoding auditory attentiona...\n",
              "4997    efficient structured matrix rank minimization ...\n",
              "4998    cient minimax signal detection graph jing qian...\n",
              "4999    signal aggregate constraint additive factorial...\n",
              "Name: paper_text, Length: 5000, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>self organization associative database applica...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mean field theory layer visual cortex applicat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>storing covariance associative long term poten...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bayesian query construction neural network mod...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>neural network ensemble cross validation activ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>low rank time frequency synthesis matthieu kow...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>state space model decoding auditory attentiona...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4997</th>\n",
              "      <td>efficient structured matrix rank minimization ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4998</th>\n",
              "      <td>cient minimax signal detection graph jing qian...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4999</th>\n",
              "      <td>signal aggregate constraint additive factorial...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5000 rows  1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e690025a",
      "metadata": {
        "id": "e690025a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "505b6d79",
      "metadata": {
        "id": "505b6d79"
      },
      "source": [
        "# Using TF-IDF\n",
        "TF-IDF stands for Text Frequency Inverse Document Frequency. The importance of each word increases in proportion to the number of times a word appears in the document (Text Frequency  TF) but is offset by the frequency of the word in the corpus (Inverse Document Frequency  IDF).\n",
        "\n",
        "Using the tf-idf weighting scheme, the keywords are the words with the highest TF-IDF score."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6879c673",
      "metadata": {
        "id": "6879c673"
      },
      "source": [
        "# CountVectorizer\n",
        "For this task, Ill first use the CountVectorizer method in Scikit-learn to create a vocabulary and generate the word count:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d93f6402",
      "metadata": {
        "id": "d93f6402"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Reduce max_features and adjust n-gram range\n",
        "cv = CountVectorizer(max_features=6000, ngram_range=(1, 2))\n",
        "\n",
        "# Create a vocabulary and word count vectors\n",
        "word_count_vectors = cv.fit_transform(docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a26d1ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "5a26d1ed",
        "outputId": "2a6cd482-c08d-48b3-8299-e407be1b8a42"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(max_features=6000, ngram_range=(1, 2))"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(max_features=6000, ngram_range=(1, 2))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;CountVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">?<span>Documentation for CountVectorizer</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>CountVectorizer(max_features=6000, ngram_range=(1, 2))</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "cv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f674697",
      "metadata": {
        "id": "7f674697"
      },
      "source": [
        "# TfidfTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e18be53",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "0e18be53",
        "outputId": "1d3966ae-d348-4b99-9dbb-420f8abd8ac2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfTransformer()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-2 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-2 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-2 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-2 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-2 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfTransformer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;TfidfTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\">?<span>Documentation for TfidfTransformer</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfTransformer()</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "tfidf_transformer.fit(word_count_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "538dd39d",
      "metadata": {
        "id": "538dd39d"
      },
      "source": [
        "# I will create a function for the task of Keyword Extraction with Python by using the Tf-IDF vectorization:m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d56fb89",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d56fb89",
        "outputId": "d1353ddd-587c-4ce8-c495-e34eba65931d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=====Title=====\n",
            "Algorithms for Non-negative Matrix Factorization\n",
            "\n",
            "=====Abstract=====\n",
            "Non-negative matrix factorization (NMF) has previously been shown to \r\n",
            "be a useful decomposition for multivariate data. Two different multi- \r\n",
            "plicative algorithms for NMF are analyzed. They differ only slightly in \r\n",
            "the multiplicative factor used in the update rules. One algorithm can be \r\n",
            "shown to minimize the conventional least squares error while the other \r\n",
            "minimizes the generalized Kullback-Leibler divergence. The monotonic \r\n",
            "convergence of both algorithms can be proven using an auxiliary func- \r\n",
            "tion analogous to that used for proving convergence of the Expectation- \r\n",
            "Maximization algorithm. The algorithms can also be interpreted as diag- \r\n",
            "onally rescaled gradient descent, where the rescaling factor is optimally \r\n",
            "chosen to ensure convergence. \n",
            "\n",
            "===Keywords===\n",
            "update rule 0.365\n",
            "update 0.317\n",
            "auxiliary 0.238\n",
            "rule 0.205\n",
            "nmf 0.196\n",
            "multiplicative 0.194\n",
            "matrix factorization 0.182\n",
            "matrix 0.176\n",
            "factorization 0.167\n",
            "non negative 0.156\n"
          ]
        }
      ],
      "source": [
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "    #taking top items from vector\n",
        "    sorted_items = sorted_items[:topn]\n",
        "\n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "    for idx, score in sorted_items:\n",
        "        fname = feature_names[idx]\n",
        "        score_vals.append(round(score,3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        "\n",
        "    #create a tuples of features,score\n",
        "    results = {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]] = score_vals[idx]  # Fix: Changed '==' to '='\n",
        "    return results\n",
        "\n",
        "\n",
        "# get feature names\n",
        "feature_names=cv.get_feature_names_out()\n",
        "\n",
        "def get_keywords(idx, docs):\n",
        "\n",
        "    #generate tf-idf for the given document\n",
        "    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs[idx]]))\n",
        "\n",
        "    #sort the tf-idf vectors by descending order of scores\n",
        "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "    #extract only the top n; n here is 10\n",
        "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
        "\n",
        "    return keywords\n",
        "\n",
        "\n",
        "def print_results(idx,keywords, df):\n",
        "    # now print the results\n",
        "    print(\"\\n=====Title=====\")\n",
        "    print(df['title'][idx])\n",
        "    print(\"\\n=====Abstract=====\")\n",
        "    print(df['abstract'][idx])\n",
        "    print(\"\\n===Keywords===\")\n",
        "    for k in keywords:\n",
        "        print(k,keywords[k])\n",
        "idx=941\n",
        "keywords=get_keywords(idx, docs)\n",
        "print_results(idx,keywords, df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff583aea",
      "metadata": {
        "id": "ff583aea"
      },
      "source": [
        "# Pickle necessary files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d2b7a8f",
      "metadata": {
        "id": "5d2b7a8f"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "pickle.dump(tfidf_transformer,open('tfidf_transformer.pkl','wb'))\n",
        "pickle.dump(cv,open('count_vectorizer.pkl','wb'))\n",
        "pickle.dump(feature_names,open('feature_names.pkl','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d676288",
      "metadata": {
        "id": "7d676288"
      },
      "outputs": [],
      "source": [
        "df['paper_text'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dcefaad",
      "metadata": {
        "id": "2dcefaad"
      },
      "outputs": [],
      "source": [
        "docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67370594",
      "metadata": {
        "id": "67370594",
        "outputId": "97f508a3-b338-42e2-9b0e-a58b0027d923"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'bayesian query construction neural network model gerhard paass jorg kindermann german national research center computer science gmd sankt augustin germany paass gmd kindermann gmd abstract data collection costly much gained actively selecting particularly informative data point sequential way bayesian decision theoretic framework develop query selection criterion explicitly take account intended use model prediction markov chain monte carlo method necessary quantity approximated desired precision number data point grows model complexity modified bayesian model selection strategy property two version criterion ate demonstrated numerical experiment introduction paper consider situation data collection costly example real measurement technical experiment performed situation approach query learning active data selection sequential experimental design etc potential benefit depending previously seen example new input value query selected systematic way corresponding output obtained motivation query learning random example often contain redundant information concentration non redundant example must necessarily improve generalization performance use bayesian decision theoretic framework derive criterion query construction criterion reflects intended use prediction appropriate gerhard paass jorg kindermann loss function limit analysis selection next data point given set data already sampled proposed procedure derives expected loss candidate input selects query minimal expected loss several published survey query construction method ford plutowski white sollich current approach cohn rely information matrix parameter however parameter receive equal attention regardless influence intended use model pronzato walter addition estimate valid asymptotically bayesian approach advocated berger applied neural network mackay sollich saad relation maximum information gain discussed paper show using markov chain monte carlo method possible determine quantity necessary selection query approach valid small sample situation procedure precision increased additional computational effort square loss function criterion reduced variant familiar integrated mean square error plutowski white next section develop query selection criterion decision theoretic point view third section show criterion calculated using markov chain monte carlo method discus strategy model selection last section result two experiment mlps described decision theoretic framework assume input vector scalar output distributed vector parameter conditional expected value deterministic function zero mean error term suppose iteratively collected observation iii get bayesian posterior predictive distribution prior distribution consider situation based data perform action whose result depends unknown output decision may severe effect others loss function measure loss true value taken action paper consider real valued action setting temperature chemical process select knowing input according bayes principle berger follow decision rule average risk minimal risk defined dydx distribution future input assumed known square loss function conditional expectation optimal decision rule control problem loss may larger specific critical point addressed weighted square loss function berger expected loss action replacing predictive density weighted predictive density bayesian query construction neural network model den den den get optimal decision rule den average loss den given input modification later derivation square loss function may applied weighted square loss aim query sampling selection new observation way average risk maximally reduced together still unknown value defines new observation new data den determine risk given perform following conceptual step candidate query future data construct possible set future observation den den future posterior determine future posterior distribution parameter den depends way though actually observed future loss assuming optimal decision rule given value compute resulting loss ylx widen dydw averaging integrate quantity future trial input distributed different possible future output yielding ylx den dxdy procedure repeated minimal average risk found since local optimum typical global optimization method required subsequently try determine whether current model still adequate whether increase complexity adding hidden unit computational procedure let assume real data den generated according regression model gaussian noise example may multilayer perceptron radial basis function network since error term independent posterior density den even case query sampling ford analytic derivation posterior infeasible except trivial case use approximation one approach employ normal approximation mackay unreliable number observation small compared number parameter use markov chain monte carlo procedure paab neal generate sample web parameter distributed according den number sampling step approach infinity distribution simulated approximates posterior arbitrarily well take account range future value create set simulation web number generated let gerhard paass jijrg kindermann resulting set instead performing new markov monte carlo run generate new sample according dcn use old set wcb parameter reweight importance sampling way may approximate integral function respect dcn kalos whitlock idcn ylx approximation error approach zero size wcb increase approximation future loss consider future loss given new observation trial input case square loss function transformed yixt dcn widcn dcn var independent assume set given representative trial input distribution define ycx equation get ylxt dcn ylx ylx dcn final value obtained averaging different ycx different trial input reduce variance trial input selected importance sampling concentrate region high current loss see facilitate search minimal reduce extent random fluctuation value let vector random number let randomly selected possible observation ycx defined wir wir way difference neighboring input affected noise search procedure exploit gradient current loss proxy future loss may use current loss rcurr dcn bayesian query construction neural network model weight input according relevance square loss function average loss conditional variance var dcn get tcurr yix dcn widcn dcn sample wcb representative dcn approximate current loss tcurr dcn input distribution uniform second term independent complexity regularization neural network model represent arbitrary mapping finite dimensional space number hidden unit sufficiently large hornik stinchcombe number observation grows hidden unit necessary catch detail mapping therefore use sequential procedure increase capacity network query learning white wooldridge call approach method sieve provide asymptotic result consistency white wooldridge gelfand dey compare bayesian approach model selection prove case nested model model choice ratio popular bayes factor dcn dcn always choose full model regardless data gelfand dey show pseudobayes factor bayesian variant crossvalidation affected paradox dcn dcn dcn difference dcn usually small use full posterior importance function get dcn ixj widcn numerical demonstration first experiment tested approach small mlp target function gaussian noise assumed square loss function uniform input distribution using true architecture approximating model started single randomly generated observation gerhard paass jijrg kindermann tuo figure future loss exploration predicted posterior mean future loss current loss observation left root mean square error prediction right estimated future loss different input selected input smallest future loss next query parameter vector generated requiring metropolis step simultaneously approximated current loss criterion left side figure show typical relation measure situation future loss low region current loss posterior standard deviation mean prediction high query concentrated area high variation estimated posterior mean approximates target function quite well right part figure rmse prediction averaged independent experiment shown observation rmse drop sharply example marked difference prediction error resulting future loss current loss criterion also averaged experiment considering substantial computing effort favor current loss criterion dot indicate rmse randomly generated data averaged experiment using bayesian prediction procedure data point located critical region high variation rmse much larger second experiment mlp defined target function gaussian noise standard deviation added shown left part figure used five mlps hidden unit candidate model generated sample web posterior pew current data started metropolis step small value increased metropolis step larger value network hidden unit observation metropolis step took second sparc workstation next used equation compare different model used optimal model calculate current loss regular grid query point assumed square loss function uniform input distribution selected query point maximal current loss determined final query point hillclimbing algorithm way rather sure get close true global optimum main result experiment summarized right part figure bayesian query construct ion neural network model exdlorati random observation figure current loss exploration mlp target function root mean square error show averaged experiment root mean square error true mean value posterior mean grid input relation sample size three phase exploration distinguished see figure beginning search performed many query border input area observation algorithm know enough detail true function concentrate relevant part input space lead marked reduction ofthe mean square error observation systematic part true function captured nearly perfectly last phase experiment algorithm merely reduces uncertainty caused random noise contrast data generated randomly sufficient information detail therefore error gradually decrease space constraint report experiment radial basis function led similar result acknowledgement work part joint project reflex german fed department science technology bmft grant number would like thank alexander linden mark ring frank weber many fruitful discussion reference berger berger statistical decision theory foundation concept method springer verlag new york cohn cohn neural network exploration using optimal experimental design cowan ed nip morgan kaufmann san mateo ford ford titterington kitsos recent advance nonlinear design technometrics gelfand dey gelfand dey bayesian model choice asymptotics exact calculation royal statistical society gerhard paass jorg kindermann figure squareroot current loss upper row absolute deviation true function lower row observation indicated dot hornik stinchcombe hornik stinchcombe multilayer feedforward network universal approximators neural network kalos whitlock kalos whitlock monte carlo method wiley new york mackay mackay information based objective function active data selection neural computation neal neal probabilistic inference using markov chain monte carlo method tech report crg dep computer science univ toronto paab paab second order probability uncertain conflicting evidence bonissone ed uncertainty artificial intelligence elsevier amsterdam plutowski white plutowski white selecting concise training set clean data ieee neural network pronzato walter pronzato walter nonsequential bayesian experimental design response optimization fedorov miiller vuchkov ed model oriented data analysis physica verlag heidelberg sollich sollich query construction entropy generalization neural network model appear physical review sollich saad sollich saad learning query maximum information gain unlearnable problem volume white wooldridge white wooldridge result sieve estimation dependent observation barnett ed nonparametric semiparametric method econometrics statistic new york cambridge univ press'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6518737",
      "metadata": {
        "id": "d6518737"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}